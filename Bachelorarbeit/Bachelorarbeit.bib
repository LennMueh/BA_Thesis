
@article{eniser_deepfault_2019,
	title = {{DeepFault}: Fault Localization for Deep Neural Networks},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1902.05974},
	doi = {10.48550/ARXIV.1902.05974},
	shorttitle = {{DeepFault}},
	abstract = {Deep Neural Networks ({DNNs}) are increasingly deployed in safety-critical applications including autonomous vehicles and medical diagnostics. To reduce the residual risk for unexpected {DNN} behaviour and provide evidence for their trustworthy operation, {DNNs} should be thoroughly tested. The {DeepFault} whitebox {DNN} testing approach presented in our paper addresses this challenge by employing suspiciousness measures inspired by fault localization to establish the hit spectrum of neurons and identify suspicious neurons whose weights have not been calibrated correctly and thus are considered responsible for inadequate {DNN} performance. {DeepFault} also uses a suspiciousness-guided algorithm to synthesize new inputs, from correctly classified inputs, that increase the activation values of suspicious neurons. Our empirical evaluation on several {DNN} instances trained on {MNIST} and {CIFAR}-10 datasets shows that {DeepFault} is effective in identifying suspicious neurons. Also, the inputs synthesized by {DeepFault} closely resemble the original inputs, exercise the identified suspicious neurons and are highly adversarial.},
	author = {Eniser, Hasan Ferit and Gerasimou, Simos and Sen, Alper},
	urldate = {2023-10-07},
	date = {2019},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Software Engineering (cs.{SE})},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\VK9U5JCX\\Eniser et al. - 2019 - DeepFault Fault Localization for Deep Neural Netw.pdf:application/pdf},
}

@software{eniser_deepfault_2023,
	title = {{DeepFault}: Fault Localization for Deep Neural Networks},
	rights = {{GPL}-3.0},
	url = {https://github.com/hfeniser/DeepFault},
	shorttitle = {{DeepFault}},
	author = {Eniser, Hasan Ferit},
	urldate = {2023-10-08},
	date = {2023-07-31},
	note = {original-date: 2018-07-04T12:51:18Z},
}

@inproceedings{jones_empirical_2005,
	location = {Long Beach {CA} {USA}},
	title = {Empirical evaluation of the tarantula automatic fault-localization technique},
	isbn = {978-1-58113-993-8},
	url = {https://dl.acm.org/doi/10.1145/1101908.1101949},
	doi = {10.1145/1101908.1101949},
	eventtitle = {{ASE}05: International Conference on Automated Software Engineering 2005},
	pages = {273--282},
	booktitle = {Proceedings of the 20th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Jones, James A. and Harrold, Mary Jean},
	urldate = {2023-10-09},
	date = {2005-11-07},
	langid = {english},
	file = {Jones und Harrold - 2005 - Empirical evaluation of the tarantula automatic fa.pdf:C\:\\Users\\lenna\\Zotero\\storage\\2UV4XGWF\\Jones und Harrold - 2005 - Empirical evaluation of the tarantula automatic fa.pdf:application/pdf},
}

@article{ochiai_zoogeographical_1957,
	title = {Zoogeographical Studies on the Soleoid Fishes found in Japan and its Neighbouring Regions-I},
	volume = {22},
	issn = {1349-998X, 0021-5392},
	url = {http://www.jstage.jst.go.jp/article/suisan1932/22/9/22_9_522/_article/-char/ja/},
	doi = {10.2331/suisan.22.522},
	pages = {522--525},
	number = {9},
	journaltitle = {{NIPPON} {SUISAN} {GAKKAISHI}},
	shortjournal = {{NIPPON} {SUISAN} {GAKKAISHI}},
	author = {Ochiai, Akira},
	urldate = {2023-10-09},
	date = {1957},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\XSWB7UBJ\\Ochiai - 1957 - Zoogeographical Studies on the Soleoid Fishes foun.pdf:application/pdf},
}

@article{wong_dstar_2014,
	title = {The {DStar} Method for Effective Software Fault Localization},
	volume = {63},
	issn = {0018-9529, 1558-1721},
	url = {http://ieeexplore.ieee.org/document/6651713/},
	doi = {10.1109/TR.2013.2285319},
	pages = {290--308},
	number = {1},
	journaltitle = {{IEEE} Transactions on Reliability},
	shortjournal = {{IEEE} Trans. Rel.},
	author = {Wong, W. Eric and Debroy, Vidroha and Gao, Ruizhi and Li, Yihao},
	urldate = {2023-10-09},
	date = {2014-03},
	file = {Wong et al. - 2014 - The DStar Method for Effective Software Fault Loca.pdf:C\:\\Users\\lenna\\Zotero\\storage\\S6CPXJEW\\Wong et al. - 2014 - The DStar Method for Effective Software Fault Loca.pdf:application/pdf},
}

@article{wong_survey_2016,
	title = {A Survey on Software Fault Localization},
	volume = {42},
	issn = {0098-5589, 1939-3520},
	url = {http://ieeexplore.ieee.org/document/7390282/},
	doi = {10.1109/TSE.2016.2521368},
	pages = {707--740},
	number = {8},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Wong, W. Eric and Gao, Ruizhi and Li, Yihao and Abreu, Rui and Wotawa, Franz},
	urldate = {2023-10-09},
	date = {2016-08-01},
	file = {Wong et al. - 2016 - A Survey on Software Fault Localization.pdf:C\:\\Users\\lenna\\Zotero\\storage\\4NBAIAMK\\Wong et al. - 2016 - A Survey on Software Fault Localization.pdf:application/pdf},
}

@article{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1708.07747},
	doi = {10.48550/ARXIV.1708.07747},
	shorttitle = {Fashion-{MNIST}},
	abstract = {We present Fashion-{MNIST}, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-{MNIST} is intended to serve as a direct drop-in replacement for the original {MNIST} dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	urldate = {2023-10-10},
	date = {2017},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\DS4TQHSW\\Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf:application/pdf},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2023-12-27},
	date = {2015-05-28},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\5ZZBBVYE\\LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation Applied to Handwritten Zip Code Recognition},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	pages = {541--551},
	number = {4},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {{LeCun}, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	urldate = {2023-12-27},
	date = {1989-12},
	langid = {english},
	file = {LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf:C\:\\Users\\lenna\\Zotero\\storage\\HXQ43KLA\\LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf:application/pdf},
}

@article{fukushima_visual_1969,
	title = {Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements},
	volume = {5},
	issn = {0536-1567},
	url = {http://ieeexplore.ieee.org/document/4082265/},
	doi = {10.1109/TSSC.1969.300225},
	pages = {322--333},
	number = {4},
	journaltitle = {{IEEE} Transactions on Systems Science and Cybernetics},
	shortjournal = {{IEEE} Trans. Syst. Sci. Cyber.},
	author = {Fukushima, Kunihiko},
	urldate = {2023-12-27},
	date = {1969},
	file = {Fukushima - 1969 - Visual Feature Extraction by a Multilayered Networ.pdf:C\:\\Users\\lenna\\Zotero\\storage\\DCZKS8S3\\Fukushima - 1969 - Visual Feature Extraction by a Multilayered Networ.pdf:application/pdf},
}

@inproceedings{yamaguchi_neural_1990,
	title = {A neural network for speaker-independent isolated word recognition},
	url = {https://www.isca-speech.org/archive/icslp_1990/yamaguchi90c_icslp.html},
	doi = {10.21437/ICSLP.1990-282},
	eventtitle = {First International Conference on Spoken Language Processing ({ICSLP} 1990)},
	pages = {1077--1080},
	booktitle = {First International Conference on Spoken Language Processing ({ICSLP} 1990)},
	publisher = {{ISCA}},
	author = {Yamaguchi, Kouichi and Sakamoto, Kenji and Akabane, Toshio and Fujimoto, Yoshiji},
	urldate = {2023-12-27},
	date = {1990-11-18},
	langid = {english},
	file = {Yamaguchi et al. - 1990 - A neural network for speaker-independent isolated .pdf:C\:\\Users\\lenna\\Zotero\\storage\\TRCWW2NB\\Yamaguchi et al. - 1990 - A neural network for speaker-independent isolated .pdf:application/pdf},
}

@misc{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	number = {{arXiv}:1412.6980},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2023-12-27},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\NPLQC9G8\\Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\BRY34MXJ\\1412.html:text/html},
}

@article{jia_analysis_2011,
	title = {An Analysis and Survey of the Development of Mutation Testing},
	volume = {37},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/5487526/},
	doi = {10.1109/TSE.2010.62},
	pages = {649--678},
	number = {5},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Jia, Yue and Harman, Mark},
	urldate = {2023-12-27},
	date = {2011-09},
	file = {Jia und Harman - 2011 - An Analysis and Survey of the Development of Mutat.pdf:C\:\\Users\\lenna\\Zotero\\storage\\Y48WEV8H\\Jia und Harman - 2011 - An Analysis and Survey of the Development of Mutat.pdf:application/pdf},
}

@article{hamlet_testing_1977,
	title = {Testing Programs with the Aid of a Compiler},
	volume = {{SE}-3},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/1702444/},
	doi = {10.1109/TSE.1977.231145},
	pages = {279--290},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Hamlet, R.G.},
	urldate = {2023-12-27},
	date = {1977-07},
	file = {Hamlet - 1977 - Testing Programs with the Aid of a Compiler.pdf:C\:\\Users\\lenna\\Zotero\\storage\\JCWB6XGX\\Hamlet - 1977 - Testing Programs with the Aid of a Compiler.pdf:application/pdf},
}

@article{demillo_hints_1978,
	title = {Hints on Test Data Selection: Help for the Practicing Programmer},
	volume = {11},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/1646911/},
	doi = {10.1109/C-M.1978.218136},
	shorttitle = {Hints on Test Data Selection},
	pages = {34--41},
	number = {4},
	journaltitle = {Computer},
	shortjournal = {Computer},
	author = {{DeMillo}, R.A. and Lipton, R.J. and Sayward, F.G.},
	urldate = {2023-12-27},
	date = {1978-04},
	file = {DeMillo et al. - 1978 - Hints on Test Data Selection Help for the Practic.pdf:C\:\\Users\\lenna\\Zotero\\storage\\IZCYZ58A\\DeMillo et al. - 1978 - Hints on Test Data Selection Help for the Practic.pdf:application/pdf},
}

@book{ammann_introduction_2017,
	location = {Cambridge, United Kingdom ; New York, {NY}, {USA}},
	edition = {Edition 2},
	title = {Introduction to software testing},
	isbn = {978-1-107-17201-2},
	pagetotal = {345},
	publisher = {Cambridge University Press},
	author = {Ammann, Paul and Offutt, Jeff},
	date = {2017},
	keywords = {Computer software, Testing},
	file = {Ammann und Offutt - 2017 - Introduction to software testing.pdf:C\:\\Users\\lenna\\Zotero\\storage\\QRVECTFY\\Ammann und Offutt - 2017 - Introduction to software testing.pdf:application/pdf},
}

@article{huang_survey_2020,
	title = {A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability},
	volume = {37},
	issn = {15740137},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574013719302527},
	doi = {10.1016/j.cosrev.2020.100270},
	shorttitle = {A survey of safety and trustworthiness of deep neural networks},
	pages = {100270},
	journaltitle = {Computer Science Review},
	shortjournal = {Computer Science Review},
	author = {Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping},
	urldate = {2023-12-27},
	date = {2020-08},
	langid = {english},
	file = {Akzeptierte Version:C\:\\Users\\lenna\\Zotero\\storage\\H9XNXWKC\\Huang et al. - 2020 - A survey of safety and trustworthiness of deep neu.pdf:application/pdf},
}

@inproceedings{guo_dlfuzz_2018,
	location = {Lake Buena Vista {FL} {USA}},
	title = {{DLFuzz}: differential fuzzing testing of deep learning systems},
	isbn = {978-1-4503-5573-5},
	url = {https://dl.acm.org/doi/10.1145/3236024.3264835},
	doi = {10.1145/3236024.3264835},
	shorttitle = {{DLFuzz}},
	eventtitle = {{ESEC}/{FSE} '18: 26th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	pages = {739--743},
	booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Guo, Jianmin and Jiang, Yu and Zhao, Yue and Chen, Quan and Sun, Jiaguang},
	urldate = {2023-12-27},
	date = {2018-10-26},
	langid = {english},
	keywords = {Fuzzing},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\YN33DLL6\\Guo et al. - 2018 - DLFuzz differential fuzzing testing of deep learn.pdf:application/pdf},
}

@article{sun_deepmc_2023,
	title = {{DeepMC}: {DNN} test sample optimization method jointly guided by misclassification and coverage},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-04323-4},
	doi = {10.1007/s10489-022-04323-4},
	shorttitle = {{DeepMC}},
	pages = {15787--15801},
	number = {12},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {Sun, Jiaze and Li, Juan and Wen, Sulei},
	urldate = {2023-12-27},
	date = {2023-06},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\LBBWCKGU\\Sun et al. - 2023 - DeepMC DNN test sample optimization method jointl.pdf:application/pdf},
}

@inproceedings{keller_critical_2017,
	location = {Prague, Czech Republic},
	title = {A Critical Evaluation of Spectrum-Based Fault Localization Techniques on a Large-Scale Software System},
	isbn = {978-1-5386-0592-9},
	url = {http://ieeexplore.ieee.org/document/8009915/},
	doi = {10.1109/QRS.2017.22},
	eventtitle = {2017 {IEEE} International Conference on Software Quality, Reliability and Security ({QRS})},
	pages = {114--125},
	booktitle = {2017 {IEEE} International Conference on Software Quality, Reliability and Security ({QRS})},
	publisher = {{IEEE}},
	author = {Keller, Fabian and Grunske, Lars and Heiden, Simon and Filieri, Antonio and Van Hoorn, Andre and Lo, David},
	urldate = {2023-12-27},
	date = {2017-07},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\87I4N8EC\\Keller et al. - 2017 - A Critical Evaluation of Spectrum-Based Fault Loca.pdf:application/pdf},
}

@article{abreu_practical_2009,
	title = {A practical evaluation of spectrum-based fault localization},
	volume = {82},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121209001319},
	doi = {10.1016/j.jss.2009.06.035},
	pages = {1780--1792},
	number = {11},
	journaltitle = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Abreu, Rui and Zoeteweij, Peter and Golsteijn, Rob and Van Gemund, Arjan J.C.},
	urldate = {2023-12-27},
	date = {2009-11},
	langid = {english},
	file = {Abreu et al. - 2009 - A practical evaluation of spectrum-based fault loc.pdf:C\:\\Users\\lenna\\Zotero\\storage\\FAQLBLV9\\Abreu et al. - 2009 - A practical evaluation of spectrum-based fault loc.pdf:application/pdf},
}

@article{sarhan_survey_2022,
	title = {A Survey of Challenges in Spectrum-Based Software Fault Localization},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9684433/},
	doi = {10.1109/ACCESS.2022.3144079},
	pages = {10618--10639},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Sarhan, Qusay Idrees and Beszedes, Arpad},
	urldate = {2023-12-27},
	date = {2022},
	file = {Sarhan und Beszedes - 2022 - A Survey of Challenges in Spectrum-Based Software .pdf:C\:\\Users\\lenna\\Zotero\\storage\\9BHL9NBR\\Sarhan und Beszedes - 2022 - A Survey of Challenges in Spectrum-Based Software .pdf:application/pdf},
}

@article{naish_model_2011,
	title = {A model for spectra-based software diagnosis},
	volume = {20},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/2000791.2000795},
	doi = {10.1145/2000791.2000795},
	abstract = {This article presents an improved approach to assist diagnosis of failures in software (fault localisation) by ranking program statements or blocks in accordance with to how likely they are to be buggy. We present a very simple single-bug program to model the problem. By examining different possible execution paths through this model program over a number of test cases, the effectiveness of different proposed spectral ranking methods can be evaluated in idealised conditions. The results are remarkably consistent to those arrived at empirically using the Siemens test suite and Space benchmarks. The model also helps identify groups of metrics that are equivalent for ranking. Due to the simplicity of the model, an optimal ranking method can be devised. This new method out-performs previously proposed methods for the model program, the Siemens test suite and Space. It also helps provide insight into other ranking methods.},
	pages = {1--32},
	number = {3},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Naish, Lee and Lee, Hua Jie and Ramamohanarao, Kotagiri},
	urldate = {2023-12-27},
	date = {2011-08},
	langid = {english},
	file = {Naish et al. - 2011 - A model for spectra-based software diagnosis.pdf:C\:\\Users\\lenna\\Zotero\\storage\\3WM8LNSW\\Naish et al. - 2011 - A model for spectra-based software diagnosis.pdf:application/pdf},
}

@inproceedings{zhou_computation_1988,
	location = {San Diego, {CA}, {USA}},
	title = {Computation of optical flow using a neural network},
	isbn = {978-0-7803-0999-9},
	url = {http://ieeexplore.ieee.org/document/23914/},
	doi = {10.1109/ICNN.1988.23914},
	eventtitle = {Proceedings of 1993 {IEEE} International Conference on Neural Networks ({ICNN} '93)},
	pages = {71--78 vol.2},
	booktitle = {{IEEE} International Conference on Neural Networks},
	publisher = {{IEEE}},
	author = {{Zhou} and {Chellappa}},
	urldate = {2023-12-30},
	date = {1988},
	file = {Zhou und Chellappa - 1988 - Computation of optical flow using a neural network.pdf:C\:\\Users\\lenna\\Zotero\\storage\\M9KSV3RV\\Zhou und Chellappa - 1988 - Computation of optical flow using a neural network.pdf:application/pdf},
}

@online{riebesell_convolution_2022,
	title = {Convolution Operator},
	url = {https://tikz.net/conv2d/},
	abstract = {Simple 2d example illustrating the role of the Jacobian determinant in the change of variables formula. Inspired by Ari Seff in https://youtu.be/i7LjDvsLWCg?t=250.},
	author = {Riebesell, Janosh},
	urldate = {2023-12-30},
	date = {2022-04-09},
	langid = {american},
	file = {Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\GM3QH9H3\\conv2d.html:text/html},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
	date = {2012},
	keywords = {⛔ No {DOI} found},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\87VBCESB\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@article{szegedy_going_2014,
	title = {Going Deeper with Convolutions},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1409.4842},
	doi = {10.48550/ARXIV.1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC} 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC} 2014 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2023-12-30},
	date = {2014},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\INBMFT48\\Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	urldate = {2023-12-30},
	date = {1986-10},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\MBVVDD4P\\Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:application/pdf},
}

@online{neutelings_neural_2021,
	title = {Neural networks},
	url = {https://tikz.net/neural_networks/},
	abstract = {Some examples of neural network architectures: deep neural networks ({DNN}), deep convolutional neural network ({CNN}), autoencoders (encoder+decoder), and activation function in neurons. Basic idea The full {LaTeX} code at the bottom of this post uses the listofitems library, so one can pre-define an array of the number of nodes in each layer, which is easier…},
	author = {Neutelings, Izaak},
	urldate = {2023-12-29},
	date = {2021-09-12},
	langid = {american},
	file = {Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\LB8JR76I\\neural_networks.html:text/html},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2023-12-29},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\WIH73JEL\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\CDKQIVSP\\1810.html:text/html},
}

@article{hendrycks_gaussian_2016,
	title = {Gaussian Error Linear Units ({GELUs})},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1606.08415},
	doi = {10.48550/ARXIV.1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit ({GELU}), a high-performing neural network activation function. The {GELU} activation function is \${xΦ}(x)\$, where \$Φ(x)\$ the standard Gaussian cumulative distribution function. The {GELU} nonlinearity weights inputs by their value, rather than gates inputs by their sign as in {ReLUs} (\$x{\textbackslash}mathbf\{1\}\_\{x\&gt;0\}\$). We perform an empirical evaluation of the {GELU} nonlinearity against the {ReLU} and {ELU} activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	urldate = {2023-12-29},
	date = {2016},
	note = {Publisher: {arXiv}
Version Number: 5},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\4LZJ5IAJ\\Hendrycks und Gimpel - 2016 - Gaussian Error Linear Units (GELUs).pdf:application/pdf},
}

@inproceedings{glorot_deep_2011,
	location = {Fort Lauderdale, {FL}, {USA}},
	title = {Deep Sparse Rectifier Neural Networks},
	volume = {15},
	url = {https://proceedings.mlr.press/v15/glorot11a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
	pages = {315--323},
	booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	editor = {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
	date = {2011-04-11},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\D5Y6EL6U\\Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf:application/pdf},
}

@article{fukushima_cognitron_1975,
	title = {Cognitron: A self-organizing multilayered neural network},
	volume = {20},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00342633},
	doi = {10.1007/BF00342633},
	shorttitle = {Cognitron},
	pages = {121--136},
	number = {3},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybernetics},
	author = {Fukushima, Kunihiko},
	urldate = {2023-12-29},
	date = {1975},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\5DZ5EF4V\\Fukushima - 1975 - Cognitron A self-organizing multilayered neural n.pdf:application/pdf},
}

@inproceedings{glorot_understanding_2010,
	location = {Chia Laguna Resort, Sardinia, Italy},
	title = {Understanding the difficulty of training deep feedforward neural networks},
	volume = {9},
	url = {https://proceedings.mlr.press/v9/glorot10a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	pages = {249--256},
	booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	editor = {Teh, Yee Whye and Titterington, Mike},
	date = {2010-05-13},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\VNRD9EEY\\Glorot und Bengio - 2010 - Understanding the difficulty of training deep feed.pdf:application/pdf},
}

@online{noauthor_tfkeraslayersdense_2023,
	title = {tf.keras.layers.Dense  {\textbar}  {TensorFlow} v2.14.0},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense},
	urldate = {2023-12-29},
	date = {2023-09-27},
	file = {tf.keras.layers.Dense  |  TensorFlow v2.14.0:C\:\\Users\\lenna\\Zotero\\storage\\CAWMREMW\\Dense.html:text/html},
}

@book{trask_grokking_2019,
	location = {Shelter Island},
	title = {Grokking deep learning},
	isbn = {978-1-61729-370-2},
	abstract = {"Grokking Deep Learning teaches you to build deep learning neural networks from scratch! In his engaging style, seasoned deep learning expert Andrew Trask shows you the science under the hood, so you grok for yourself every detail of training neural networks. Using only Python and its math-supporting library, {NumPy}, you'll train your own neural networks to see and understand images, translate text into different languages, and even write like Shakespeare!"--},
	pagetotal = {309},
	publisher = {Manning},
	author = {Trask, Andrew W.},
	date = {2019},
	note = {{OCLC}: on1084981313},
	keywords = {Machine learning, Neural networks (Computer science), Python (Computer program language)},
	file = {Trask - 2019 - Grokking deep learning.pdf:C\:\\Users\\lenna\\Zotero\\storage\\BPLD3I2J\\Trask - 2019 - Grokking deep learning.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	location = {Cambridge, Massachusetts},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	series = {Adaptive computation and machine learning},
	pagetotal = {775},
	publisher = {The {MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016},
	keywords = {Machine learning},
	file = {Goodfellow et al. - 2016 - Deep learning.pdf:C\:\\Users\\lenna\\Zotero\\storage\\MIZTQGIU\\Goodfellow et al. - 2016 - Deep learning.pdf:application/pdf},
}

@inproceedings{boureau_theoretical_2010,
	title = {A theoretical analysis of feature pooling in visual recognition},
	pages = {111--118},
	booktitle = {Proceedings of the 27th international conference on machine learning ({ICML}-10)},
	author = {Boureau, Y-Lan and Ponce, Jean and {LeCun}, Yann},
	date = {2010},
	keywords = {⛔ No {DOI} found},
	file = {Boureau et al. - 2010 - A theoretical analysis of feature pooling in visua.pdf:C\:\\Users\\lenna\\Zotero\\storage\\87LRD8XW\\Boureau et al. - 2010 - A theoretical analysis of feature pooling in visua.pdf:application/pdf},
}

@inproceedings{jain_supervised_2007,
	location = {Rio de Janeiro, Brazil},
	title = {Supervised Learning of Image Restoration with Convolutional Networks},
	isbn = {978-1-4244-1630-1},
	url = {http://ieeexplore.ieee.org/document/4408909/},
	doi = {10.1109/ICCV.2007.4408909},
	eventtitle = {2007 {IEEE} 11th International Conference on Computer Vision},
	pages = {1--8},
	booktitle = {2007 {IEEE} 11th International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Jain, Viren and Murray, Joseph F. and Roth, Fabian and Turaga, Srinivas and Zhigulin, Valentin and Briggman, Kevin L. and Helmstaedter, Moritz N. and Denk, Winfried and Seung, H. Sebastian},
	urldate = {2024-01-03},
	date = {2007},
	file = {Jain et al. - 2007 - Supervised Learning of Image Restoration with Conv.pdf:C\:\\Users\\lenna\\Zotero\\storage\\PHT4F9BL\\Jain et al. - 2007 - Supervised Learning of Image Restoration with Conv.pdf:application/pdf},
}

@inproceedings{boureau_ask_2011,
	location = {Barcelona, Spain},
	title = {Ask the locals: Multi-way local pooling for image recognition},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	url = {http://ieeexplore.ieee.org/document/6126555/},
	doi = {10.1109/ICCV.2011.6126555},
	shorttitle = {Ask the locals},
	eventtitle = {2011 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {2651--2658},
	booktitle = {2011 International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Boureau, Y-Lan and Le Roux, Nicolas and Bach, Francis and Ponce, Jean and {LeCun}, Yann},
	urldate = {2024-01-03},
	date = {2011-11},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\QE3P8UIB\\Boureau et al. - 2011 - Ask the locals Multi-way local pooling for image .pdf:application/pdf},
}

@inproceedings{zhou_computation_1988-1,
	location = {San Diego, {CA}, {USA}},
	title = {Computation of optical flow using a neural network},
	isbn = {978-0-7803-0999-9},
	url = {http://ieeexplore.ieee.org/document/23914/},
	doi = {10.1109/ICNN.1988.23914},
	eventtitle = {Proceedings of 1993 {IEEE} International Conference on Neural Networks ({ICNN} '93)},
	pages = {71--78 vol.2},
	booktitle = {{IEEE} International Conference on Neural Networks},
	publisher = {{IEEE}},
	author = {{Zhou} and {Chellappa}},
	urldate = {2024-01-03},
	date = {1988},
	file = {Zhou und Chellappa - 1988 - Computation of optical flow using a neural network.pdf:C\:\\Users\\lenna\\Zotero\\storage\\YL4LKQ62\\Zhou und Chellappa - 1988 - Computation of optical flow using a neural network.pdf:application/pdf},
}

@inproceedings{zhou_stereo_1988,
	location = {New York, {NY}, {USA}},
	title = {Stereo matching using a neural network},
	url = {http://ieeexplore.ieee.org/document/196745/},
	doi = {10.1109/ICASSP.1988.196745},
	eventtitle = {{ICASSP}-88., International Conference on Acoustics, Speech, and Signal Processing},
	pages = {940--943},
	booktitle = {{ICASSP}-88., International Conference on Acoustics, Speech, and Signal Processing},
	publisher = {{IEEE}},
	author = {Zhou, Y.T. and Chellappa, R.},
	urldate = {2024-01-03},
	date = {1988},
}

@article{kingma_adam_2014,
	title = {Adam: A Method for Stochastic Optimization},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1412.6980},
	doi = {10.48550/ARXIV.1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2024-01-03},
	date = {2014},
	note = {Publisher: {arXiv}
Version Number: 9},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\AJRQB6IZ\\Kingma und Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@article{huang_survey_2020-1,
	title = {A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability},
	volume = {37},
	issn = {15740137},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574013719302527},
	doi = {10.1016/j.cosrev.2020.100270},
	shorttitle = {A survey of safety and trustworthiness of deep neural networks},
	pages = {100270},
	journaltitle = {Computer Science Review},
	shortjournal = {Computer Science Review},
	author = {Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping},
	urldate = {2024-01-04},
	date = {2020-08},
	langid = {english},
	file = {1-s2.0-S1574013719302527-main.pdf:C\:\\Users\\lenna\\Zotero\\storage\\AIY9VV5Q\\1-s2.0-S1574013719302527-main.pdf:application/pdf;Akzeptierte Version:C\:\\Users\\lenna\\Zotero\\storage\\WLNYWWSM\\Huang et al. - 2020 - A survey of safety and trustworthiness of deep neu.pdf:application/pdf},
}

@incollection{beyer_feature-guided_2018,
	location = {Cham},
	title = {Feature-Guided Black-Box Safety Testing of Deep Neural Networks},
	volume = {10805},
	isbn = {978-3-319-89959-6 978-3-319-89960-2},
	url = {http://link.springer.com/10.1007/978-3-319-89960-2_22},
	pages = {408--426},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	publisher = {Springer International Publishing},
	author = {Wicker, Matthew and Huang, Xiaowei and Kwiatkowska, Marta},
	editor = {Beyer, Dirk and Huisman, Marieke},
	urldate = {2024-01-04},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-89960-2_22},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Input Mutation, Safety Coverage},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\IG5JW6PX\\Wicker et al. - 2018 - Feature-Guided Black-Box Safety Testing of Deep Ne.pdf:application/pdf},
}

@misc{odena_tensorfuzz_2018,
	title = {{TensorFuzz}: Debugging Neural Networks with Coverage-Guided Fuzzing},
	url = {http://arxiv.org/abs/1807.10875},
	shorttitle = {{TensorFuzz}},
	abstract = {Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing ({CGF}) methods for neural networks. In {CGF}, random mutations of inputs to a neural network are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how fast approximate nearest neighbor algorithms can provide this coverage metric. We then discuss the application of {CGF} to the following goals: finding numerical errors in trained neural networks, generating disagreements between neural networks and quantized versions of those networks, and surfacing undesirable behavior in character level language models. Finally, we release an open source library called {TensorFuzz} that implements the described techniques.},
	number = {{arXiv}:1807.10875},
	publisher = {{arXiv}},
	author = {Odena, Augustus and Goodfellow, Ian},
	urldate = {2024-01-04},
	date = {2018-07-27},
	eprinttype = {arxiv},
	eprint = {1807.10875 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Fuzzing},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\VZYA9ED4\\Odena und Goodfellow - 2018 - TensorFuzz Debugging Neural Networks with Coverag.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\6VTYPRUZ\\1807.html:text/html},
}

@misc{gopinath_symbolic_2018,
	title = {Symbolic Execution for Deep Neural Networks},
	url = {http://arxiv.org/abs/1807.10439},
	abstract = {Deep Neural Networks ({DNN}) are increasingly used in a variety of applications, many of them with substantial safety and security concerns. This paper introduces {DeepCheck}, a new approach for validating {DNNs} based on core ideas from program analysis, specifically from symbolic execution. The idea is to translate a {DNN} into an imperative program, thereby enabling program analysis to assist with {DNN} validation. A basic translation however creates programs that are very complex to analyze. {DeepCheck} introduces novel techniques for lightweight symbolic analysis of {DNNs} and applies them in the context of image classification to address two challenging problems in {DNN} analysis: 1) identification of important pixels (for attribution and adversarial generation); and 2) creation of 1-pixel and 2-pixel attacks. Experimental results using the {MNIST} data-set show that {DeepCheck}'s lightweight symbolic analysis provides a valuable tool for {DNN} validation.},
	number = {{arXiv}:1807.10439},
	publisher = {{arXiv}},
	author = {Gopinath, Divya and Wang, Kaiyuan and Zhang, Mengshi and Pasareanu, Corina S. and Khurshid, Sarfraz},
	urldate = {2024-01-04},
	date = {2018-07-27},
	eprinttype = {arxiv},
	eprint = {1807.10439 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Cryptography and Security, Symbolic Execution},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\XS6PZIRH\\Gopinath et al. - 2018 - Symbolic Execution for Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\I7XZLNY4\\1807.html:text/html},
}

@inproceedings{sun_deepconcolic_2019,
	location = {Montreal, {QC}, Canada},
	title = {{DeepConcolic}: Testing and Debugging Deep Neural Networks},
	isbn = {978-1-72811-764-5},
	url = {https://ieeexplore.ieee.org/document/8802786/},
	doi = {10.1109/ICSE-Companion.2019.00051},
	shorttitle = {{DeepConcolic}},
	eventtitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	pages = {111--114},
	booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	publisher = {{IEEE}},
	author = {Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Sharp, James and Hill, Matthew and Ashmore, Rob},
	urldate = {2024-01-04},
	date = {2019-05},
	keywords = {Symbolic Execution, Neuron Coverage},
	file = {Sun et al. - 2019 - DeepConcolic Testing and Debugging Deep Neural Ne.pdf:C\:\\Users\\lenna\\Zotero\\storage\\K8KK573D\\Sun et al. - 2019 - DeepConcolic Testing and Debugging Deep Neural Ne.pdf:application/pdf},
}

@report{hayhurst_practical_2001,
	title = {A Practical Tutorial on Modified Condition/Decision Coverage},
	url = {https://ntrs.nasa.gov/citations/20010057789},
	abstract = {This tutorial provides a practical approach to assessing modified condition/decision coverage ({MC}/{DC}) for aviation software products that must comply with regulatory guidance for {DO}-178B level A software. The tutorial's approach to {MC}/{DC} is a 5-step process that allows a certification authority or verification analyst to evaluate {MC}/{DC} claims without the aid of a coverage tool. In addition to the {MC}/{DC} approach, the tutorial addresses factors to consider in selecting and qualifying a structural coverage analysis tool, tips for reviewing life cycle data related to {MC}/{DC}, and pitfalls common to structural coverage analysis.},
	number = {L-18088},
	author = {Hayhurst, Kelly J. and Veerhusen, Dan S. and Chilenski, John J. and Rierson, Leanna K.},
	urldate = {2024-01-04},
	date = {2001-05-01},
	note = {{NTRS} Author Affiliations: {NASA} Langley Research Center, Rockwell Collins, Inc., Boeing Co., Federal Aviation Administration
{NTRS} Document {ID}: 20010057789
{NTRS} Research Center: Langley Research Center ({LaRC})},
	keywords = {Computer Programming And Software},
	file = {Hayhurst et al. - 2001 - A Practical Tutorial on Modified ConditionDecisio.pdf:C\:\\Users\\lenna\\Zotero\\storage\\6CXKM9MN\\Hayhurst et al. - 2001 - A Practical Tutorial on Modified ConditionDecisio.pdf:application/pdf;Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\DZGLMH26\\20010057789.html:text/html},
}

@incollection{lahiri_quantitative_2018,
	location = {Cham},
	title = {Quantitative Projection Coverage for Testing {ML}-enabled Autonomous Systems},
	volume = {11138},
	isbn = {978-3-030-01089-8 978-3-030-01090-4},
	url = {http://link.springer.com/10.1007/978-3-030-01090-4_8},
	pages = {126--142},
	booktitle = {Automated Technology for Verification and Analysis},
	publisher = {Springer International Publishing},
	author = {Cheng, Chih-Hong and Huang, Chung-Hao and Yasuoka, Hirotoshi},
	editor = {Lahiri, Shuvendu K. and Wang, Chao},
	urldate = {2024-01-04},
	date = {2018},
	doi = {10.1007/978-3-030-01090-4_8},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Quantitative projection coverage},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\2HWT8P6Q\\Cheng et al. - 2018 - Quantitative Projection Coverage for Testing ML-en.pdf:application/pdf},
}

@collection{wong_handbook_2023,
	location = {Hoboken, New Jersey, Piscataway, {NJ}},
	title = {Handbook of software fault localization: foundations and advances},
	isbn = {978-1-119-88092-9},
	shorttitle = {Handbook of software fault localization},
	abstract = {"Software fault localization is one of the most expensive activities in program debugging. It can be further divided into two major parts. The first part is to use a technique to identify suspicious code that may contain program bugs. The second part is for programmers to actually examine the identified code to decide whether it indeed contains bugs. Fault localization has historically been a manual task that has been recognized to be time consuming and tedious as well as prohibitively expensive, given the size and complexity of large-scale software systems today. Furthermore, manual fault localization relies heavily on the software developer's experience, judgment, and intuition to identify and prioritize code that is likely to be faulty. These limitations have led to a surge of interest in developing techniques that can partially or fully automate the localization of faults in software while reducing human input"-- Provided by publisher},
	publisher = {John Wiley \& Sons, Inc ; {IEEE} Press},
	editor = {Wong, W. Eric and Tse, T. H.},
	date = {2023},
	note = {{OCLC}: 1345466980},
	file = {Wong und Tse - 2023 - Handbook of software fault localization foundatio.pdf:C\:\\Users\\lenna\\Zotero\\storage\\WUGFAFA2\\Wong und Tse - 2023 - Handbook of software fault localization foundatio.pdf:application/pdf},
}

@book{parsa_software_2023,
	location = {Cham},
	title = {Software testing automation: testability evaluation, refactoring, test data generation and fault localization},
	isbn = {978-3-031-22057-9},
	shorttitle = {Software testing automation},
	abstract = {This book is about the design and development of tools for software testing. It intends to get the reader involved in software testing rather than simply memorizing the concepts. The source codes are downloadable from the book website. The book has three parts: software testability, fault localization, and test data generation. Part I describes unit and acceptance tests and proposes a new method called testability-driven development ({TsDD}) in support of {TDD} and {BDD}. {TsDD} uses a machine learning model to measure testability before and after refactoring. The reader will learn how to develop the testability prediction model and write software tools for automatic refactoring. Part {II} focuses on developing tools for automatic fault localization. This part shows the reader how to use a compiler generator to instrument source code, create control flow graphs, identify prime paths, and slice the source code. On top of these tools, a software tool, Diagnoser, is offered to facilitate experimenting with and developing new fault localization algorithms. Diagnoser takes a source code and its test suite as input and reports the coverage provided by the test cases and the suspiciousness score for each statement. Part {III} proposes using software testing as a prominent part of the cyber-physical system software to uncover and model unknown physical behaviors and the underlying physical rules. The reader will get insights into developing software tools to generate white box test data.},
	publisher = {Springer},
	author = {Parsa, Saeed},
	date = {2023},
	note = {{OCLC}: 1374251026},
	file = {Parsa - 2023 - Software testing automation testability evaluatio.pdf:C\:\\Users\\lenna\\Zotero\\storage\\7H47FSF9\\Parsa - 2023 - Software testing automation testability evaluatio.pdf:application/pdf},
}

@book{xie_essential_2021,
	location = {Singapore},
	title = {Essential spectrum-based fault localization},
	isbn = {978-981-336-179-9},
	abstract = {Program debugging has always been a difficult and time-consuming task in the context of software development, where spectrum-based fault localization ({SBFL}) is one of the most widely studied families of techniques. While it's not particularly difficult to learn about the process and empirical performance of a particular {SBFL} technique from the available literature, researchers and practitioners aren't always familiar with the underlying theories. This book provides the first comprehensive guide to fundamental theories in {SBFL}, while also addressing some emerging challenges in this area. The theoretical framework introduced here reveals the intrinsic relations between various risk evaluation formulas, making it possible to construct a formula performance hierarchy. Further extensions of the framework provide a sufficient and necessary condition for a general maximal formula, as well as performance comparisons for hybrid {SBFL} methods. With regard to emerging challenges in {SBFL}, the book mainly covers the frequently encountered oracle problem in {SBFL} and introduces a metamorphic slice-based solution. In addition, it discusses the challenge of multiple-fault localization and presents cutting-edge approaches to overcoming it. {SBFL} is a widely studied research area with a massive amount of publications. Thus, it is essential that the software engineering community, especially those involved in program debugging, software maintenance and software quality assurance (including both newcomers and researchers who want to gain deeper insights) understand the most fundamental theories - which could also be very helpful to ensuring the healthy development of the field},
	publisher = {Springer},
	author = {Xie, Xiaoyuan and Xu, Baowen},
	date = {2021},
	note = {{OCLC}: 1236368200},
	file = {Xie und Xu - 2021 - Essential spectrum-based fault localization.pdf:C\:\\Users\\lenna\\Zotero\\storage\\P43NLHC3\\Xie und Xu - 2021 - Essential spectrum-based fault localization.pdf:application/pdf},
}

@inproceedings{hua_jie_lee_study_2009,
	location = {Beijing, China},
	title = {Study of the relationship of bug consistency with respect to performance of spectra metrics},
	isbn = {978-1-4244-4519-6},
	url = {http://ieeexplore.ieee.org/document/5234512/},
	doi = {10.1109/ICCSIT.2009.5234512},
	eventtitle = {2009 2nd {IEEE} International Conference on Computer Science and Information Technology},
	pages = {501--508},
	booktitle = {2009 2nd {IEEE} International Conference on Computer Science and Information Technology},
	publisher = {{IEEE}},
	author = {{Hua Jie Lee} and Naish, Lee and {Kotagiri Ramamohanarao}},
	urldate = {2024-01-05},
	date = {2009},
	keywords = {Trantula metric symplified.},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\2Q6V9238\\Hua Jie Lee et al. - 2009 - Study of the relationship of bug consistency with .pdf:application/pdf},
}

@article{wong_survey_2016-1,
	title = {A Survey on Software Fault Localization},
	volume = {42},
	issn = {0098-5589, 1939-3520},
	url = {http://ieeexplore.ieee.org/document/7390282/},
	doi = {10.1109/TSE.2016.2521368},
	pages = {707--740},
	number = {8},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Wong, W. Eric and Gao, Ruizhi and Li, Yihao and Abreu, Rui and Wotawa, Franz},
	urldate = {2024-01-05},
	date = {2016-08-01},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\LBVSZYZ7\\Wong et al. - 2016 - A Survey on Software Fault Localization.pdf:application/pdf},
}

@article{yoo_human_2017,
	title = {Human Competitiveness of Genetic Programming in Spectrum-Based Fault Localisation: Theoretical and Empirical Analysis},
	volume = {26},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3078840},
	doi = {10.1145/3078840},
	shorttitle = {Human Competitiveness of Genetic Programming in Spectrum-Based Fault Localisation},
	abstract = {We report on the application of Genetic Programming to Software Fault Localisation, a problem in the area of Search-Based Software Engineering ({SBSE}). We give both empirical and theoretical evidence for the human competitiveness of the evolved fault localisation formulæ under the single fault scenario, compared to those generated by human ingenuity and reported in many papers, published over more than a decade. Though there have been previous human competitive results claimed for {SBSE} problems, this is the first time that evolved solutions have been formally proved to be human competitive. We further prove that no future human investigation could outperform the evolved solutions. We complement these proofs with an empirical analysis of both human and evolved solutions, which indicates that the evolved solutions are not only theoretically human competitive, but also convey similar practical benefits to human-evolved counterparts.},
	pages = {1--30},
	number = {1},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Yoo, Shin and Xie, Xiaoyuan and Kuo, Fei-Ching and Chen, Tsong Yueh and Harman, Mark},
	urldate = {2024-01-05},
	date = {2017-01-31},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\SCKXPB3H\\Yoo et al. - 2017 - Human Competitiveness of Genetic Programming in Sp.pdf:application/pdf},
}

@inproceedings{petrovic_industrial_2018,
	location = {Vasteras},
	title = {An Industrial Application of Mutation Testing: Lessons, Challenges, and Research Directions},
	isbn = {978-1-5386-6352-3},
	url = {https://ieeexplore.ieee.org/document/8411730/},
	doi = {10.1109/ICSTW.2018.00027},
	shorttitle = {An Industrial Application of Mutation Testing},
	eventtitle = {2018 {IEEE} International Conference on Software Testing, Verification and Validation Workshops ({ICSTW})},
	pages = {47--53},
	booktitle = {2018 {IEEE} International Conference on Software Testing, Verification and Validation Workshops ({ICSTW})},
	publisher = {{IEEE}},
	author = {Petrovic, Goran and Ivankovic, Marko and Kurtz, Bob and Ammann, Paul and Just, Rene},
	urldate = {2024-01-07},
	date = {2018-04},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\M2TFJF2Q\\Petrovic et al. - 2018 - An Industrial Application of Mutation Testing Les.pdf:application/pdf},
}

@article{belli_model-based_2016,
	title = {Model-based mutation testing—Approach and case studies},
	volume = {120},
	issn = {01676423},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167642316000137},
	doi = {10.1016/j.scico.2016.01.003},
	pages = {25--48},
	journaltitle = {Science of Computer Programming},
	shortjournal = {Science of Computer Programming},
	author = {Belli, Fevzi and Budnik, Christof J. and Hollmann, Axel and Tuglular, Tugkan and Wong, W. Eric},
	urldate = {2024-01-07},
	date = {2016-05},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\Y33LLHM8\\Belli et al. - 2016 - Model-based mutation testing—Approach and case stu.pdf:application/pdf;Volltext:C\:\\Users\\lenna\\Zotero\\storage\\YY26U7GU\\Belli et al. - 2016 - Model-based mutation testing—Approach and case stu.pdf:application/pdf},
}

@book{jorgensen_software_2021,
	location = {Boca Raton, {FL} London New York},
	edition = {Fifth edititon},
	title = {Software testing: a craftsman's approach},
	isbn = {978-0-367-35849-5 978-0-367-76762-4},
	series = {An Auerbach book},
	shorttitle = {Software testing},
	pagetotal = {529},
	publisher = {{CRC} Press, Taylor \& Francis Group},
	author = {Jorgensen, Paul C. and {DeVries}, Byron},
	date = {2021},
	file = {Jorgensen und DeVries - 2021 - Software testing a craftsman's approach.pdf:C\:\\Users\\lenna\\Zotero\\storage\\D3QKGLC3\\Jorgensen und DeVries - 2021 - Software testing a craftsman's approach.pdf:application/pdf},
}

@thesis{heitmuller_user_2023,
	title = {User Partitioning for Anytime Local-Search {MaxSAT} Solvers},
	institution = {{TU} Hamburg},
	type = {Bachelor Thesis},
	author = {Heitmüller, Jan},
	date = {2023-10},
	langid = {english},
	file = {Heitmüller - 2023 - User Partitioning for Anytime Local-Search MaxSAT .pdf:C\:\\Users\\lenna\\Zotero\\storage\\76G8Y2SL\\Heitmüller - 2023 - User Partitioning for Anytime Local-Search MaxSAT .pdf:application/pdf},
}

@online{noauthor_build_2023,
	title = {Build from source on Windows {\textbar} {TensorFlow}},
	url = {https://www.tensorflow.org/install/source_windows},
	urldate = {2024-01-11},
	date = {2023-11-21},
	langid = {english},
	file = {Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\FGTTEVX3\\source_windows.html:text/html},
}

@misc{martin_abadi_tensorflow_2015,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	date = {2015},
}

@article{sohn_arachne_2023,
	title = {Arachne: Search-Based Repair of Deep Neural Networks},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3563210},
	doi = {10.1145/3563210},
	shorttitle = {Arachne},
	abstract = {The rapid and widespread adoption of Deep Neural Networks ({DNNs}) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of {DNNs}. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This article introduces Arachne, a novel program repair technique for {DNNs}, which directly repairs {DNNs} using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses differential evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a {DNN} without reducing general accuracy significantly. On average, patches generated by Arachne generalise to
              61.3\%
              of unseen misbehaviour, whereas those by a state-of-the-art {DNN} repair technique generalise only to
              10.2\% and sometimes to none while taking tens of times more than Arachne.
              We also show that Arachne can address fairness issues by debiasing a gender classification model.
              Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond convolutional neural networks.},
	pages = {1--26},
	number = {4},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Sohn, Jeongju and Kang, Sungmin and Yoo, Shin},
	urldate = {2024-01-12},
	date = {2023-10-31},
	langid = {english},
	keywords = {Wichtig},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\6KGVTB6A\\Sohn et al. - 2023 - Arachne Search-Based Repair of Deep Neural Networ.pdf:application/pdf},
}

@inproceedings{zhang_apricot_2019,
	location = {San Diego, {CA}, {USA}},
	title = {Apricot: A Weight-Adaptation Approach to Fixing Deep Learning Models},
	isbn = {978-1-72812-508-4},
	url = {https://ieeexplore.ieee.org/document/8952197/},
	doi = {10.1109/ASE.2019.00043},
	shorttitle = {Apricot},
	eventtitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {376--387},
	booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Zhang, Hao and Chan, W.K.},
	urldate = {2024-01-12},
	date = {2019-11},
	keywords = {Wichtig},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\UBIY8MVK\\Zhang und Chan - 2019 - Apricot A Weight-Adaptation Approach to Fixing De.pdf:application/pdf},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	pages = {2825--2830},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	date = {2011},
	keywords = {⛔ No {DOI} found},
}

@online{noauthor_what_nodate,
	title = {What is the wording for the declaration to be included in each thesis?},
	url = {https://www.tf.uni-kiel.de/servicezentrum/en/examination-office-mawi/faq/what-is-the-wording-for-the-declaration-to-be-included-in-each-thesis},
	abstract = {What is the wording for the declaration to be included in each thesis?},
	titleaddon = {Service},
	urldate = {2024-01-13},
	langid = {english},
	file = {Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\2Z3LDCRC\\what-is-the-wording-for-the-declaration-to-be-included-in-each-thesis.html:text/html},
}

@inproceedings{kim_repairing_2023,
	location = {Dublin, Ireland},
	title = {Repairing {DNN} Architecture: Are We There Yet?},
	isbn = {978-1-66545-666-1},
	url = {https://ieeexplore.ieee.org/document/10132163/},
	doi = {10.1109/ICST57152.2023.00030},
	shorttitle = {Repairing {DNN} Architecture},
	eventtitle = {2023 {IEEE} Conference on Software Testing, Verification and Validation ({ICST})},
	pages = {234--245},
	booktitle = {2023 {IEEE} Conference on Software Testing, Verification and Validation ({ICST})},
	publisher = {{IEEE}},
	author = {Kim, Jinhan and Humbatova, Nargiz and Jahangirova, Gunel and Tonella, Paolo and Yoo, Shin},
	urldate = {2024-01-14},
	date = {2023-04},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\55LMCYYW\\Kim et al. - 2023 - Repairing DNN Architecture Are We There Yet.pdf:application/pdf},
}

@inproceedings{ma_deepmutation_2018,
	location = {Memphis, {TN}},
	title = {{DeepMutation}: Mutation Testing of Deep Learning Systems},
	isbn = {978-1-5386-8321-7},
	url = {https://ieeexplore.ieee.org/document/8539073/},
	doi = {10.1109/ISSRE.2018.00021},
	shorttitle = {{DeepMutation}},
	eventtitle = {2018 {IEEE} 29th International Symposium on Software Reliability Engineering ({ISSRE})},
	pages = {100--111},
	booktitle = {2018 {IEEE} 29th International Symposium on Software Reliability Engineering ({ISSRE})},
	publisher = {{IEEE}},
	author = {Ma, Lei and Zhang, Fuyuan and Sun, Jiyuan and Xue, Minhui and Li, Bo and Juefei-Xu, Felix and Xie, Chao and Li, Li and Liu, Yang and Zhao, Jianjun and Wang, Yadong},
	urldate = {2024-01-14},
	date = {2018-10},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\SDG92D72\\Ma et al. - 2018 - DeepMutation Mutation Testing of Deep Learning Sy.pdf:application/pdf},
}

@inproceedings{ghanbari_mutation-based_2023,
	location = {Luxembourg, Luxembourg},
	title = {Mutation-based Fault Localization of Deep Neural Networks},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298394/},
	doi = {10.1109/ASE56229.2023.00171},
	eventtitle = {2023 38th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {1301--1313},
	booktitle = {2023 38th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Ghanbari, Ali and Thomas, Deepak-George and Arshad, Muhammad Arbab and Rajan, Hridesh},
	urldate = {2024-01-14},
	date = {2023-09-11},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\M6T27KWE\\Ghanbari et al. - 2023 - Mutation-based Fault Localization of Deep Neural N.pdf:application/pdf},
}

@article{islam_repairing_2020,
	title = {Repairing Deep Neural Networks: Fix Patterns and Challenges},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2005.00972},
	doi = {10.48550/ARXIV.2005.00972},
	shorttitle = {Repairing Deep Neural Networks},
	abstract = {Significant interest in applying Deep Neural Network ({DNN}) has fueled the need to support engineering of software that uses {DNNs}. Repairing software that uses {DNNs} is one such unmistakable {SE} need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing {DNNs}. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack overflow and 555 repairs from Github for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that {DNN} bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; {DNN} bug fixes have the potential to introduce adversarial vulnerabilities; {DNN} bug fixes frequently introduce new bugs; and {DNN} bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 {DNN} (bug, repair) instances.},
	author = {Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh},
	urldate = {2024-01-14},
	date = {2020},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {{FOS}: Computer and information sciences, Software Engineering (cs.{SE})},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\VB64MWP6\\Islam et al. - 2020 - Repairing Deep Neural Networks Fix Patterns and C.pdf:application/pdf},
}

@article{wang_cnn_2021,
	title = {{CNN} Explainer: Learning Convolutional Neural Networks with Interactive Visualization},
	volume = {27},
	issn = {1077-2626, 1941-0506, 2160-9306},
	url = {https://ieeexplore.ieee.org/document/9222325/},
	doi = {10.1109/TVCG.2020.3030418},
	shorttitle = {{CNN} Explainer},
	pages = {1396--1406},
	number = {2},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	shortjournal = {{IEEE} Trans. Visual. Comput. Graphics},
	author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Polo Chau, Duen Horng},
	urldate = {2024-01-14},
	date = {2021-02},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\EX2Q4TX4\\Wang et al. - 2021 - CNN Explainer Learning Convolutional Neural Netwo.pdf:application/pdf},
}

@report{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex},
	date = {2009},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	url = {https://doi.org/10.1038/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	pages = {357--362},
	number = {7825},
	journaltitle = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and Walt, Stéfan J. van der and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and Kerkwijk, Marten H. van and Brett, Matthew and Haldane, Allan and Río, Jaime Fernández del and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	date = {2020-09},
	note = {Publisher: Springer Science and Business Media {LLC}},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\YQAGCECY\\Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf},
}

@software{collette_h5pyh5py_2022,
	title = {h5py/h5py: 3.7.0},
	rights = {Open Access},
	url = {https://zenodo.org/record/6575970},
	shorttitle = {h5py/h5py},
	abstract = {{HDF}5 for Python -- The h5py package is a Pythonic interface to the {HDF}5 binary data format.},
	version = {3.7.0},
	publisher = {Zenodo},
	author = {Collette, Andrew and Kluyver, Thomas and Caswell, Thomas A and Tocknell, James and Kieffer, Jerome and Jelenak, Aleksandar and Scopatz, Anthony and Dale, Darren and Chen and VINCENT, Thomas and Einhorn, Matt and {Payno} and {Juliagarriga} and {Pierlauro Sciarelli} and Valls, Valentin and {Satrajit Ghosh} and Pedersen, Ulrik Kofoed and {Jakirkham} and Raspaud, Martin and Danilevski, Cyril and {Hameer Abbasi} and Readey, John and Mühlbauer, Kai and Paramonov, Andrey and Chan, Lawrence and {V. Armando Solé} and {Jialin} and Guest, Daniel Hay and Feng, Yu and Kittisopikul, Mark},
	urldate = {2024-01-25},
	date = {2022-05-24},
	doi = {10.5281/ZENODO.6575970},
}

@inproceedings{pei_deepxplore_2017,
	title = {{DeepXplore}: Automated Whitebox Testing of Deep Learning Systems},
	url = {http://arxiv.org/abs/1705.06640},
	doi = {10.1145/3132747.3132785},
	shorttitle = {{DeepXplore}},
	abstract = {Deep learning ({DL}) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing {DL} testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs. We design, implement, and evaluate {DeepXplore}, the first whitebox framework for systematically testing real-world {DL} systems. First, we introduce neuron coverage for systematically measuring the parts of a {DL} system exercised by test inputs. Next, we leverage multiple {DL} systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for {DL} systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques. {DeepXplore} efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art {DL} models with thousands of neurons trained on five popular datasets including {ImageNet} and Udacity self-driving challenge data. For all tested {DL} models, on average, {DeepXplore} generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by {DeepXplore} can also be used to retrain the corresponding {DL} model to improve the model's accuracy by up to 3\%.},
	pages = {1--18},
	booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
	author = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
	urldate = {2024-02-01},
	date = {2017-10-14},
	eprinttype = {arxiv},
	eprint = {1705.06640 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\LYWZE5BV\\Pei et al. - 2017 - DeepXplore Automated Whitebox Testing of Deep Lea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\5HZQKJT8\\1705.html:text/html},
}

@incollection{beyer_feature-guided_2018-1,
	location = {Cham},
	title = {Feature-Guided Black-Box Safety Testing of Deep Neural Networks},
	volume = {10805},
	isbn = {978-3-319-89959-6 978-3-319-89960-2},
	url = {http://link.springer.com/10.1007/978-3-319-89960-2_22},
	pages = {408--426},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	publisher = {Springer International Publishing},
	author = {Wicker, Matthew and Huang, Xiaowei and Kwiatkowska, Marta},
	editor = {Beyer, Dirk and Huisman, Marieke},
	urldate = {2024-02-01},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-89960-2_22},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\BX8C2488\\Wicker et al. - 2018 - Feature-Guided Black-Box Safety Testing of Deep Ne.pdf:application/pdf},
}

@inproceedings{cheng_manifesting_2018,
	location = {Lisbon},
	title = {Manifesting Bugs in Machine Learning Code: An Explorative Study with Mutation Testing},
	isbn = {978-1-5386-7757-5},
	url = {https://ieeexplore.ieee.org/document/8424982/},
	doi = {10.1109/QRS.2018.00044},
	shorttitle = {Manifesting Bugs in Machine Learning Code},
	eventtitle = {2018 {IEEE} International Conference on Software Quality, Reliability and Security ({QRS})},
	pages = {313--324},
	booktitle = {2018 {IEEE} International Conference on Software Quality, Reliability and Security ({QRS})},
	publisher = {{IEEE}},
	author = {Cheng, Dawei and Cao, Chun and Xu, Chang and Ma, Xiaoxing},
	urldate = {2024-02-01},
	date = {2018-07},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\KYXFVGGJ\\Cheng et al. - 2018 - Manifesting Bugs in Machine Learning Code An Expl.pdf:application/pdf},
}

@software{chollet_keras_2015,
	title = {Keras},
	url = {https://keras.io},
	author = {Chollet, François and {others}},
	date = {2015},
}

@inproceedings{dong_towards_2021,
	location = {Hainan, China},
	title = {Towards Repairing Neural Networks Correctly},
	isbn = {978-1-66545-813-9},
	url = {https://ieeexplore.ieee.org/document/9724879/},
	doi = {10.1109/QRS54544.2021.00081},
	eventtitle = {2021 {IEEE} 21st International Conference on Software Quality, Reliability and Security ({QRS})},
	pages = {714--725},
	booktitle = {2021 {IEEE} 21st International Conference on Software Quality, Reliability and Security ({QRS})},
	publisher = {{IEEE}},
	author = {Dong, Guoliang and Sun, Jun and Wang, Xingen and Wang, Xinyu and Dai, Ting},
	urldate = {2024-02-23},
	date = {2021-12},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\7J4MW5UU\\Dong et al. - 2021 - Towards Repairing Neural Networks Correctly.pdf:application/pdf},
}

@article{khan_regularization_2019,
	title = {Regularization of deep neural networks with spectral dropout},
	volume = {110},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608018302715},
	doi = {10.1016/j.neunet.2018.09.009},
	pages = {82--90},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Khan, Salman H. and Hayat, Munawar and Porikli, Fatih},
	urldate = {2024-02-24},
	date = {2019-02},
	langid = {english},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\LKR4LFR5\\Khan et al. - 2019 - Regularization of deep neural networks with spectr.pdf:application/pdf},
}

@article{noulapeu_ngaffo_deep_2022,
	title = {A deep neural network-based collaborative filtering using a matrix factorization with a twofold regularization},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-021-06831-9},
	doi = {10.1007/s00521-021-06831-9},
	pages = {6991--7003},
	number = {9},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Noulapeu Ngaffo, Armielle and Choukair, Zièd},
	urldate = {2024-02-24},
	date = {2022-05},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\SJ8Y5SAG\\Noulapeu Ngaffo und Choukair - 2022 - A deep neural network-based collaborative filterin.pdf:application/pdf},
}

@article{chatzikonstantinou_recurrent_2021,
	title = {Recurrent neural network pruning using dynamical systems and iterative fine-tuning},
	volume = {143},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608021002641},
	doi = {10.1016/j.neunet.2021.07.001},
	pages = {475--488},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Chatzikonstantinou, Christos and Konstantinidis, Dimitrios and Dimitropoulos, Kosmas and Daras, Petros},
	urldate = {2024-02-24},
	date = {2021-11},
	langid = {english},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\Y2VHTXVH\\Chatzikonstantinou et al. - 2021 - Recurrent neural network pruning using dynamical s.pdf:application/pdf},
}

@article{moradi_survey_2020,
	title = {A survey of regularization strategies for deep models},
	volume = {53},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-019-09784-7},
	doi = {10.1007/s10462-019-09784-7},
	pages = {3947--3986},
	number = {6},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Moradi, Reza and Berangi, Reza and Minaei, Behrouz},
	urldate = {2024-02-24},
	date = {2020-08},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\MSPVEWLI\\Moradi et al. - 2020 - A survey of regularization strategies for deep mod.pdf:application/pdf},
}

@article{nuti_evidence-based_2022,
	title = {Evidence-Based Regularization for Neural Networks},
	volume = {4},
	issn = {2504-4990},
	url = {https://www.mdpi.com/2504-4990/4/4/51},
	doi = {10.3390/make4040051},
	abstract = {Numerous approaches address over-fitting in neural networks: by imposing a penalty on the parameters of the network (L1, L2, etc.); by changing the network stochastically (drop-out, Gaussian noise, etc.); or by transforming the input data (batch normalization, etc.). In contrast, we aim to ensure that a minimum amount of supporting evidence is present when fitting the model parameters to the training data. This, at the single neuron level, is equivalent to ensuring that both sides of the separating hyperplane (for a standard artificial neuron) have a minimum number of data points, noting that these points need not belong to the same class for the inner layers. We firstly benchmark the results of this approach on the standard Fashion-{MINST} dataset, comparing it to various regularization techniques. Interestingly, we note that by nudging each neuron to divide, at least in part, its input data, the resulting networks make use of each neuron, avoiding a hyperplane completely on one side of its input data (which is equivalent to a constant into the next layers). To illustrate this point, we study the prevalence of saturated nodes throughout training, showing that neurons are activated more frequently and earlier in training when using this regularization approach. A direct consequence of the improved neuron activation is that deep networks are now easier to train. This is crucially important when the network topology is not known a priori and fitting often remains stuck in a suboptimal local minima. We demonstrate this property by training a network of increasing depth (and constant width); most regularization approaches will result in increasingly frequent training failures (over different random seeds), whilst the proposed evidence-based regularization significantly outperforms in its ability to train deep networks.},
	pages = {1011--1023},
	number = {4},
	journaltitle = {Machine Learning and Knowledge Extraction},
	shortjournal = {{MAKE}},
	author = {Nuti, Giuseppe and Cross, Andreea-Ingrid and Rindler, Philipp},
	urldate = {2024-02-24},
	date = {2022-11-15},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\WQ9VJ4NE\\Nuti et al. - 2022 - Evidence-Based Regularization for Neural Networks.pdf:application/pdf},
}

@article{singh_calibrating_2021,
	title = {Calibrating feature maps for deep {CNNs}},
	volume = {438},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220320312},
	doi = {10.1016/j.neucom.2020.12.119},
	pages = {235--247},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Singh, Pravendra and Mazumder, Pratik and Karim, Mohammed Asad and Namboodiri, Vinay P.},
	urldate = {2024-02-24},
	date = {2021-05},
	langid = {english},
	file = {Singh et al. - 2021 - Calibrating feature maps for deep CNNs.pdf:C\:\\Users\\lenna\\Zotero\\storage\\KDYNHGU7\\Singh et al. - 2021 - Calibrating feature maps for deep CNNs.pdf:application/pdf},
}

@article{guo_weak_2021,
	title = {Weak sub-network pruning for strong and efficient neural networks},
	volume = {144},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608021003658},
	doi = {10.1016/j.neunet.2021.09.015},
	pages = {614--626},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Guo, Qingbei and Wu, Xiao-Jun and Kittler, Josef and Feng, Zhiquan},
	urldate = {2024-02-24},
	date = {2021-12},
	langid = {english},
	file = {Guo et al. - 2021 - Weak sub-network pruning for strong and efficient .pdf:C\:\\Users\\lenna\\Zotero\\storage\\7F94AZ8E\\Guo et al. - 2021 - Weak sub-network pruning for strong and efficient .pdf:application/pdf},
}

@article{liang_pruning_2021,
	title = {Pruning and quantization for deep neural network acceleration: A survey},
	volume = {461},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221010894},
	doi = {10.1016/j.neucom.2021.07.045},
	shorttitle = {Pruning and quantization for deep neural network acceleration},
	pages = {370--403},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
	urldate = {2024-02-24},
	date = {2021-10},
	langid = {english},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\RRXDCF4C\\Liang et al. - 2021 - Pruning and quantization for deep neural network a.pdf:application/pdf},
}

@article{yeom_pruning_2021,
	title = {Pruning by explaining: A novel criterion for deep neural network pruning},
	volume = {115},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321000868},
	doi = {10.1016/j.patcog.2021.107899},
	shorttitle = {Pruning by explaining},
	pages = {107899},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Yeom, Seul-Ki and Seegerer, Philipp and Lapuschkin, Sebastian and Binder, Alexander and Wiedemann, Simon and Müller, Klaus-Robert and Samek, Wojciech},
	urldate = {2024-02-24},
	date = {2021-07},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\WMJ2BVSB\\Yeom et al. - 2021 - Pruning by explaining A novel criterion for deep .pdf:application/pdf},
}

@article{maru_software_2021,
	title = {Software fault localization using {BP} neural network based on function and branch coverage},
	volume = {14},
	issn = {1864-5909, 1864-5917},
	url = {http://link.springer.com/10.1007/s12065-019-00318-2},
	doi = {10.1007/s12065-019-00318-2},
	pages = {87--104},
	number = {1},
	journaltitle = {Evolutionary Intelligence},
	shortjournal = {Evol. Intel.},
	author = {Maru, Abha and Dutta, Arpita and Kumar, K. Vinod and Mohapatra, Durga Prasad},
	urldate = {2024-02-24},
	date = {2021-03},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\TK98XIZZ\\Maru et al. - 2021 - Software fault localization using BP neural networ.pdf:application/pdf},
}

@inproceedings{hssayni_novel_2020,
	location = {Fez, Morocco},
	title = {A Novel Pooling Method for Regularization of Deep Neural networks},
	isbn = {978-1-72818-041-0},
	url = {https://ieeexplore.ieee.org/document/9204322/},
	doi = {10.1109/ISCV49265.2020.9204322},
	eventtitle = {2020 International Conference on Intelligent Systems and Computer Vision ({ISCV})},
	pages = {1--6},
	booktitle = {2020 International Conference on Intelligent Systems and Computer Vision ({ISCV})},
	publisher = {{IEEE}},
	author = {Hssayni, El Houssaine and Ettaouil, Mohamed},
	urldate = {2024-02-24},
	date = {2020-06},
	file = {Hssayni und Ettaouil - 2020 - A Novel Pooling Method for Regularization of Deep .pdf:C\:\\Users\\lenna\\Zotero\\storage\\S4J6WA57\\Hssayni und Ettaouil - 2020 - A Novel Pooling Method for Regularization of Deep .pdf:application/pdf},
}

@inproceedings{badola_analysis_2020,
	location = {New Delhi, India},
	title = {An Analysis of Regularization Methods in Deep Neural Networks},
	isbn = {978-1-72816-916-3},
	url = {https://ieeexplore.ieee.org/document/9342192/},
	doi = {10.1109/INDICON49873.2020.9342192},
	eventtitle = {2020 {IEEE} 17th India Council International Conference ({INDICON})},
	pages = {1--6},
	booktitle = {2020 {IEEE} 17th India Council International Conference ({INDICON})},
	publisher = {{IEEE}},
	author = {Badola, Akshay and Nair, Vineet Padmanabhan and Lal, Rajendra Prasad},
	urldate = {2024-02-24},
	date = {2020-12-10},
	file = {Badola et al. - 2020 - An Analysis of Regularization Methods in Deep Neur.pdf:C\:\\Users\\lenna\\Zotero\\storage\\BUSYSB2L\\Badola et al. - 2020 - An Analysis of Regularization Methods in Deep Neur.pdf:application/pdf},
}

@inproceedings{wu_mutation-based_2021,
	location = {Yinchuan, China},
	title = {A Mutation-based Approach to Repair Deep Neural Network Models},
	isbn = {978-1-66544-391-3},
	url = {https://ieeexplore.ieee.org/document/9622974/},
	doi = {10.1109/DSA52907.2021.00106},
	eventtitle = {2021 8th International Conference on Dependable Systems and Their Applications ({DSA})},
	pages = {730--731},
	booktitle = {2021 8th International Conference on Dependable Systems and Their Applications ({DSA})},
	publisher = {{IEEE}},
	author = {Wu, Huanhuan and Li, Zheng and Cui, Zhanqi and Zhang, Jiaming},
	urldate = {2024-02-24},
	date = {2021-08},
	file = {Wu et al. - 2021 - A Mutation-based Approach to Repair Deep Neural Ne.pdf:C\:\\Users\\lenna\\Zotero\\storage\\869E96IQ\\Wu et al. - 2021 - A Mutation-based Approach to Repair Deep Neural Ne.pdf:application/pdf},
}

@inproceedings{song_airepair_2023,
	location = {Melbourne, Australia},
	title = {{AIREPAIR}: A Repair Platform for Neural Networks},
	isbn = {9798350322637},
	url = {https://ieeexplore.ieee.org/document/10172571/},
	doi = {10.1109/ICSE-Companion58688.2023.00033},
	shorttitle = {{AIREPAIR}},
	eventtitle = {2023 {IEEE}/{ACM} 45th International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	pages = {98--101},
	booktitle = {2023 {IEEE}/{ACM} 45th International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	publisher = {{IEEE}},
	author = {Song, Xidan and Sun, Youcheng and Mustafa, Mustafa A. and Cordeiro, Lucas C.},
	urldate = {2024-02-24},
	date = {2023-05},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\BTTADHMB\\Song et al. - 2023 - AIREPAIR A Repair Platform for Neural Networks.pdf:application/pdf},
}

@article{yu_deeprepair_2022,
	title = {\textit{{DeepRepair}:} Style-Guided Repairing for Deep Neural Networks in the Real-World Operational Environment},
	volume = {71},
	issn = {0018-9529, 1558-1721},
	url = {https://ieeexplore.ieee.org/document/9508369/},
	doi = {10.1109/TR.2021.3096332},
	shorttitle = {{\textless}i{\textgreater}{DeepRepair}},
	pages = {1401--1416},
	number = {4},
	journaltitle = {{IEEE} Transactions on Reliability},
	shortjournal = {{IEEE} Trans. Rel.},
	author = {Yu, Bing and Qi, Hua and Guo, Qing and Juefei-Xu, Felix and Xie, Xiaofei and Ma, Lei and Zhao, Jianjun},
	urldate = {2024-02-24},
	date = {2022-12},
	file = {Yu et al. - 2022 - DeepRepair Style-Guided Repairing for Deep.pdf:C\:\\Users\\lenna\\Zotero\\storage\\X9ZVUX32\\Yu et al. - 2022 - DeepRepair Style-Guided Repairing for Deep.pdf:application/pdf},
}

@misc{balderas_optimizing_2023,
	title = {Optimizing Convolutional Neural Network Architecture},
	url = {http://arxiv.org/abs/2401.01361},
	abstract = {Convolutional Neural Networks ({CNN}) are widely used to face challenging tasks like speech recognition, natural language processing or computer vision. As {CNN} architectures get larger and more complex, their computational requirements increase, incurring significant energetic costs and challenging their deployment on resource-restricted devices. In this paper, we propose Optimizing Convolutional Neural Network Architecture ({OCNNA}), a novel {CNN} optimization and construction method based on pruning and knowledge distillation designed to establish the importance of convolutional layers. The proposal has been evaluated though a thorough empirical study including the best known datasets ({CIFAR}-10, {CIFAR}-100 and Imagenet) and {CNN} architectures ({VGG}-16, {ResNet}-50, {DenseNet}-40 and {MobileNet}), setting Accuracy Drop and Remaining Parameters Ratio as objective metrics to compare the performance of {OCNNA} against the other state-of-art approaches. Our method has been compared with more than 20 convolutional neural network simplification algorithms obtaining outstanding results. As a result, {OCNNA} is a competitive {CNN} constructing method which could ease the deployment of neural networks into {IoT} or resource-limited devices.},
	number = {{arXiv}:2401.01361},
	publisher = {{arXiv}},
	author = {Balderas, Luis and Lastra, Miguel and Benítez, José M.},
	urldate = {2024-02-24},
	date = {2023-12-17},
	eprinttype = {arxiv},
	eprint = {2401.01361 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\FE9L55EW\\Balderas et al. - 2023 - Optimizing Convolutional Neural Network Architectu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\QU2SFLIN\\2401.html:text/html},
}

@article{santos_avoiding_2022,
	title = {Avoiding Overfitting: A Survey on Regularization Methods for Convolutional Neural Networks},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3510413},
	doi = {10.1145/3510413},
	shorttitle = {Avoiding Overfitting},
	abstract = {Several image processing tasks, such as image classification and object detection, have been significantly improved using Convolutional Neural Networks ({CNN}). Like {ResNet} and {EfficientNet}, many architectures have achieved outstanding results in at least one dataset by the time of their creation. A critical factor in training concerns the network’s regularization, which prevents the structure from overfitting. This work analyzes several regularization methods developed in the past few years, showing significant improvements for different {CNN} models. The works are classified into three main areas: the first one is called “data augmentation,” where all the techniques focus on performing changes in the input data. The second, named “internal changes,” aims to describe procedures to modify the feature maps generated by the neural network or the kernels. The last one, called “label,” concerns transforming the labels of a given input. This work presents two main differences comparing to other available surveys about regularization: (i) the first concerns the papers gathered in the manuscript, which are not older than five years, and (ii) the second distinction is about reproducibility, i.e., all works referred here have their code available in public repositories or they have been directly implemented in some framework, such as {TensorFlow} or Torch.},
	pages = {1--25},
	number = {10},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Santos, Claudio Filipi Gonçalves Dos and Papa, João Paulo},
	urldate = {2024-02-24},
	date = {2022-01-31},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\QSR2FKBU\\Santos und Papa - 2022 - Avoiding Overfitting A Survey on Regularization M.pdf:application/pdf},
}

@inproceedings{ma_mode_2018,
	location = {Lake Buena Vista {FL} {USA}},
	title = {{MODE}: automated neural network model debugging via state differential analysis and input selection},
	isbn = {978-1-4503-5573-5},
	url = {https://dl.acm.org/doi/10.1145/3236024.3236082},
	doi = {10.1145/3236024.3236082},
	shorttitle = {{MODE}},
	eventtitle = {{ESEC}/{FSE} '18: 26th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	pages = {175--186},
	booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Ma, Shiqing and Liu, Yingqi and Lee, Wen-Chuan and Zhang, Xiangyu and Grama, Ananth},
	urldate = {2024-02-24},
	date = {2018-10-26},
	langid = {english},
	file = {Ma et al. - 2018 - MODE automated neural network model debugging via.pdf:C\:\\Users\\lenna\\Zotero\\storage\\DJAUJX3V\\Ma et al. - 2018 - MODE automated neural network model debugging via.pdf:application/pdf},
}

@article{qi_archrepair_2023,
	title = {\textit{{ArchRepair}} : Block-Level Architecture-Oriented Repairing for Deep Neural Networks},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3585005},
	doi = {10.1145/3585005},
	shorttitle = {\textit{{ArchRepair}}},
	abstract = {Over the past few years, deep neural networks ({DNNs}) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, {DNNs} are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair {DNNs} for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair {DNNs} by jointly optimizing the architecture and weights at a higher (i.e., block level).
            
              We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for {DNN} repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e.,
              block localization
              , where we should localize the targeted block that we need to fix; and how to perform
              joint architecture and weight repairing
              . Specifically, we first propose
              adversarial-aware spectrum analysis for vulnerable block localization
              that considers the neurons’ status and weights’ gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the
              architecture-oriented search-based repairing
              that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named
              {ArchRepair}
              , and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art {DNN} repair techniques.},
	pages = {1--31},
	number = {5},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Qi, Hua and Wang, Zhijie and Guo, Qing and Chen, Jianlang and Juefei-Xu, Felix and Zhang, Fuyuan and Ma, Lei and Zhao, Jianjun},
	urldate = {2024-02-24},
	date = {2023-09-30},
	langid = {english},
	file = {Qi et al. - 2023 - ArchRepair  Block-Level Architecture-Orien.pdf:C\:\\Users\\lenna\\Zotero\\storage\\QQA7BAXY\\Qi et al. - 2023 - ArchRepair  Block-Level Architecture-Orien.pdf:application/pdf},
}

@inproceedings{henriksen_repairing_2022,
	location = {Virtual Event},
	title = {Repairing misclassifications in neural networks using limited data},
	isbn = {978-1-4503-8713-2},
	url = {https://dl.acm.org/doi/10.1145/3477314.3507059},
	doi = {10.1145/3477314.3507059},
	eventtitle = {{SAC} '22: The 37th {ACM}/{SIGAPP} Symposium on Applied Computing},
	pages = {1031--1038},
	booktitle = {Proceedings of the 37th {ACM}/{SIGAPP} Symposium on Applied Computing},
	publisher = {{ACM}},
	author = {Henriksen, Patrick and Leofante, Francesco and Lomuscio, Alessio},
	urldate = {2024-02-24},
	date = {2022-04-25},
	langid = {english},
	file = {Henriksen et al. - 2022 - Repairing misclassifications in neural networks us.pdf:C\:\\Users\\lenna\\Zotero\\storage\\W79UQZFS\\Henriksen et al. - 2022 - Repairing misclassifications in neural networks us.pdf:application/pdf},
}

@inproceedings{nakagawa_experience_2023,
	location = {Taipa, Macao},
	title = {An Experience Report on Regression-Free Repair of Deep Neural Network Model},
	isbn = {978-1-66545-278-6},
	url = {https://ieeexplore.ieee.org/document/10123626/},
	doi = {10.1109/SANER56733.2023.00090},
	eventtitle = {2023 {IEEE} International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	pages = {778--782},
	booktitle = {2023 {IEEE} International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	publisher = {{IEEE}},
	author = {Nakagawa, Takao and Tokumoto, Susumu and Tokui, Shogo and Ishikawa, Fuyuki},
	urldate = {2024-02-24},
	date = {2023-03},
	file = {Nakagawa et al. - 2023 - An Experience Report on Regression-Free Repair of .pdf:C\:\\Users\\lenna\\Zotero\\storage\\MPF62GZG\\Nakagawa et al. - 2023 - An Experience Report on Regression-Free Repair of .pdf:application/pdf},
}

@inproceedings{li_calsi_adaptive_2023,
	location = {Lisbon Portugal},
	title = {Adaptive Search-based Repair of Deep Neural Networks},
	isbn = {9798400701191},
	url = {https://dl.acm.org/doi/10.1145/3583131.3590477},
	doi = {10.1145/3583131.3590477},
	eventtitle = {{GECCO} '23: Genetic and Evolutionary Computation Conference},
	pages = {1527--1536},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	publisher = {{ACM}},
	author = {Li Calsi, Davide and Duran, Matias and Laurent, Thomas and Zhang, Xiao-Yi and Arcaini, Paolo and Ishikawa, Fuyuki},
	urldate = {2024-02-24},
	date = {2023-07-15},
	langid = {english},
	file = {Li Calsi et al. - 2023 - Adaptive Search-based Repair of Deep Neural Networ.pdf:C\:\\Users\\lenna\\Zotero\\storage\\3UGFKXA9\\Li Calsi et al. - 2023 - Adaptive Search-based Repair of Deep Neural Networ.pdf:application/pdf},
}

@inproceedings{munakata_towards_2023,
	location = {Melbourne, Australia},
	title = {Towards Formal Repair and Verification of Industry-scale Deep Neural Networks},
	isbn = {9798350322637},
	url = {https://ieeexplore.ieee.org/document/10172532/},
	doi = {10.1109/ICSE-Companion58688.2023.00103},
	eventtitle = {2023 {IEEE}/{ACM} 45th International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	pages = {360--364},
	booktitle = {2023 {IEEE}/{ACM} 45th International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	publisher = {{IEEE}},
	author = {Munakata, Satoshi and Tokumoto, Susumu and Yamamoto, Koji and Munakata, Kazuki},
	urldate = {2024-02-24},
	date = {2023-05},
	file = {Towards_Formal_Repair_and_Verification_of_Industry-scale_Deep_Neural_Networks.pdf:C\:\\Users\\lenna\\Zotero\\storage\\BDAX4HJM\\Towards_Formal_Repair_and_Verification_of_Industry-scale_Deep_Neural_Networks.pdf:application/pdf},
}

@inproceedings{humbatova_deepcrime_2023,
	location = {Melbourne, Australia},
	title = {{DeepCrime}: from Real Faults to Mutation Testing Tool for Deep Learning},
	isbn = {9798350322637},
	url = {https://ieeexplore.ieee.org/document/10172493/},
	doi = {10.1109/ICSE-Companion58688.2023.00027},
	shorttitle = {{DeepCrime}},
	eventtitle = {2023 {IEEE}/{ACM} 45th International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	pages = {68--72},
	booktitle = {2023 {IEEE}/{ACM} 45th International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	publisher = {{IEEE}},
	author = {Humbatova, Nargiz and Jahangirova, Gunel and Tonella, Paolo},
	urldate = {2024-02-24},
	date = {2023-05},
	file = {Humbatova et al. - 2023 - DeepCrime from Real Faults to Mutation Testing To.pdf:C\:\\Users\\lenna\\Zotero\\storage\\N9EXLL2J\\Humbatova et al. - 2023 - DeepCrime from Real Faults to Mutation Testing To.pdf:application/pdf},
}

@inproceedings{yahmed_intentional_2023,
	title = {An Intentional Forgetting-Driven Self-Healing Method for Deep Reinforcement Learning Systems},
	doi = {10.1109/ASE56229.2023.00121},
	pages = {1314--1325},
	booktitle = {2023 38th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Yahmed, Ahmed Haj and Bouchoucha, Rached and Braiek, Houssem Ben and Khomh, Foutse},
	date = {2023},
	keywords = {Behavioral sciences, Continual Learning, Deep learning, Deep Reinforcement Learning, Intentional Forgetting, Neurons, Production, Reinforcement learning, Social networking (online), Software Healing, Training},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\N3668FSF\\Yahmed et al. - 2023 - An Intentional Forgetting-Driven Self-Healing Meth.pdf:application/pdf},
}

@inproceedings{ishimoto_initial_2023,
	title = {An Initial Analysis of Repair and Side-effect Prediction for Neural Networks},
	doi = {10.1109/CAIN58948.2023.00017},
	pages = {80--85},
	booktitle = {2023 {IEEE}/{ACM} 2nd International Conference on {AI} Engineering – Software Engineering for {AI} ({CAIN})},
	author = {Ishimoto, Yuta and Matsui, Ken and Kondo, Masanari and Ubayashi, Naoyasu and Kamei, Yasutaka},
	date = {2023},
	keywords = {Artificial intelligence, Costs, Maintenance engineering, neural network, Neural networks, Predictive models, Quality assurance, repair, side-effect, software engineering for {AI}, Software systems},
	file = {Ishimoto et al. - 2023 - An Initial Analysis of Repair and Side-effect Pred.pdf:C\:\\Users\\lenna\\Zotero\\storage\\L7SNX3NA\\Ishimoto et al. - 2023 - An Initial Analysis of Repair and Side-effect Pred.pdf:application/pdf},
}

@inproceedings{calsi_distributed_2023,
	title = {Distributed Repair of Deep Neural Networks},
	doi = {10.1109/ICST57152.2023.00017},
	pages = {83--94},
	booktitle = {2023 {IEEE} Conference on Software Testing, Verification and Validation ({ICST})},
	author = {Calsi, Davide Li and Duran, Matias and Zhang, Xiao-Yi and Arcaini, Paolo and Ishikawa, Fuyuki},
	date = {2023},
	keywords = {automated repair, Collaboration, Computational modeling, Deep learning, {DNNs}, Neural networks, risk levels, Scalability, Software algorithms, Software testing},
	file = {Calsi et al. - 2023 - Distributed Repair of Deep Neural Networks.pdf:C\:\\Users\\lenna\\Zotero\\storage\\YN8QXA4Q\\Calsi et al. - 2023 - Distributed Repair of Deep Neural Networks.pdf:application/pdf},
}

@inproceedings{zhang_plum_2021,
	title = {Plum: Exploration and Prioritization of Model Repair Strategies for Fixing Deep Learning Models},
	doi = {10.1109/DSA52907.2021.00023},
	pages = {140--151},
	booktitle = {2021 8th International Conference on Dependable Systems and Their Applications ({DSA})},
	author = {Zhang, Hao and Chan, W.K.},
	date = {2021},
	keywords = {Deep learning, Training, Maintenance engineering, Debugging, Deep neural networks, hyperheuristic, Model evolution, Model repair, Phase measurement, Strategy prioritization},
	file = {Zhang und Chan - 2021 - Plum Exploration and Prioritization of Model Repa.pdf:C\:\\Users\\lenna\\Zotero\\storage\\LHR62BCN\\Zhang und Chan - 2021 - Plum Exploration and Prioritization of Model Repa.pdf:application/pdf},
}

@inproceedings{wardat_deeplocalize_2021,
	title = {{DeepLocalize}: Fault Localization for Deep Neural Networks},
	doi = {10.1109/ICSE43902.2021.00034},
	pages = {251--262},
	booktitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering ({ICSE})},
	author = {Wardat, Mohammad and Le, Wei and Rajan, Hridesh},
	date = {2021},
	keywords = {Benchmark testing, Computer bugs, Debugging, Deep learning bugs, Deep Neural Networks, Fault diagnosis, Fault Location, Neural networks, Numerical models, Program Analysis, Tools},
	file = {Eingereichte Version:C\:\\Users\\lenna\\Zotero\\storage\\SIT7LTG5\\Wardat et al. - 2021 - DeepLocalize Fault Localization for Deep Neural N.pdf:application/pdf},
}

@misc{tokui_neurecover_2022,
	title = {{NeuRecover}: Regression-Controlled Repair of Deep Neural Networks with Training History},
	url = {http://arxiv.org/abs/2203.00191},
	shorttitle = {{NeuRecover}},
	abstract = {Systematic techniques to improve quality of deep neural networks ({DNNs}) are critical given the increasing demand for practical applications including safety-critical ones. The key challenge comes from the little controllability in updating {DNNs}. Retraining to fix some behavior often has a destructive impact on other behavior, causing regressions, i.e., the updated {DNN} fails with inputs correctly handled by the original one. This problem is crucial when engineers are required to investigate failures in intensive assurance activities for safety or trust. Search-based repair techniques for {DNNs} have potentials to tackle this challenge by enabling localized updates only on "responsible parameters" inside the {DNN}. However, the potentials have not been explored to realize sufficient controllability to suppress regressions in {DNN} repair tasks. In this paper, we propose a novel {DNN} repair method that makes use of the training history for judging which {DNN} parameters should be changed or not to suppress regressions. We implemented the method into a tool called {NeuRecover} and evaluated it with three datasets. Our method outperformed the existing method by achieving often less than a quarter, even a tenth in some cases, number of regressions. Our method is especially effective when the repair requirements are tight to fix specific failure types. In such cases, our method showed stably low rates ({\textless}2\%) of regressions, which were in many cases a tenth of regressions caused by retraining.},
	number = {{arXiv}:2203.00191},
	publisher = {{arXiv}},
	author = {Tokui, Shogo and Tokumoto, Susumu and Yoshii, Akihito and Ishikawa, Fuyuki and Nakagawa, Takao and Munakata, Kazuki and Kikuchi, Shinji},
	urldate = {2024-02-25},
	date = {2022-03-04},
	eprinttype = {arxiv},
	eprint = {2203.00191 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\YGUQ2587\\Tokui et al. - 2022 - NeuRecover Regression-Controlled Repair of Deep N.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\FG47AZZD\\2203.html:text/html},
}

@article{cowen-rivers_hebo_2020,
	title = {{HEBO} Pushing The Limits of Sample-Efficient Hyperparameter Optimisation},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2012.03826},
	doi = {10.48550/ARXIV.2012.03826},
	abstract = {In this work we rigorously analyse assumptions inherent to black-box optimisation hyper-parameter tuning tasks. Our results on the Bayesmark benchmark indicate that heteroscedasticity and non-stationarity pose significant challenges for black-box optimisers. Based on these findings, we propose a Heteroscedastic and Evolutionary Bayesian Optimisation solver ({HEBO}). {HEBO} performs non-linear input and output warping, admits exact marginal log-likelihood optimisation and is robust to the values of learned parameters. We demonstrate {HEBO}'s empirical efficacy on the {NeurIPS} 2020 Black-Box Optimisation challenge, where {HEBO} placed first. Upon further analysis, we observe that {HEBO} significantly outperforms existing black-box optimisers on 108 machine learning hyperparameter tuning tasks comprising the Bayesmark benchmark. Our findings indicate that the majority of hyper-parameter tuning tasks exhibit heteroscedasticity and non-stationarity, multi-objective acquisition ensembles with Pareto front solutions improve queried configurations, and robust acquisition maximisers afford empirical advantages relative to their non-robust counterparts. We hope these findings may serve as guiding principles for practitioners of Bayesian optimisation. All code is made available at https://github.com/huawei-noah/{HEBO}.},
	author = {Cowen-Rivers, Alexander I. and Lyu, Wenlong and Tutunov, Rasul and Wang, Zhi and Grosnit, Antoine and Griffiths, Ryan Rhys and Maraval, Alexandre Max and Jianye, Hao and Wang, Jun and Peters, Jan and Ammar, Haitham Bou},
	urldate = {2024-02-25},
	date = {2020},
	note = {Publisher: {arXiv}
Version Number: 6},
	keywords = {{FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Optimization and Control (math.{OC})},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\Y82CCC4U\\Cowen-Rivers et al. - 2020 - HEBO Pushing The Limits of Sample-Efficient Hyperp.pdf:application/pdf},
}

@article{falkner_bohb_2018,
	title = {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1807.01774},
	doi = {10.48550/ARXIV.1807.01774},
	shorttitle = {{BOHB}},
	abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
	author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
	urldate = {2024-02-25},
	date = {2018},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\H73FIEML\\Falkner et al. - 2018 - BOHB Robust and Efficient Hyperparameter Optimiza.pdf:application/pdf},
}

@inproceedings{zhang_autotrainer_2021,
	location = {Madrid, {ES}},
	title = {{AUTOTRAINER}: An Automatic {DNN} Training Problem Detection and Repair System},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9402077/},
	doi = {10.1109/ICSE43902.2021.00043},
	shorttitle = {{AUTOTRAINER}},
	eventtitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering ({ICSE})},
	pages = {359--371},
	booktitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering ({ICSE})},
	publisher = {{IEEE}},
	author = {Zhang, Xiaoyu and Zhai, Juan and Ma, Shiqing and Shen, Chao},
	urldate = {2024-02-25},
	date = {2021-05},
	file = {Zhang et al. - 2021 - AUTOTRAINER An Automatic DNN Training Problem Det.pdf:C\:\\Users\\lenna\\Zotero\\storage\\ANLAHBXG\\Zhang et al. - 2021 - AUTOTRAINER An Automatic DNN Training Problem Det.pdf:application/pdf},
}
