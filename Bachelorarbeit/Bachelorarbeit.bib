@article{eniser_deepfault_2019,
	title = {{DeepFault}: Fault Localization for Deep Neural Networks},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1902.05974},
	shorttitle = {{DeepFault}},
	abstract = {Deep Neural Networks ({DNNs}) are increasingly deployed in safety-critical applications including autonomous vehicles and medical diagnostics. To reduce the residual risk for unexpected {DNN} behaviour and provide evidence for their trustworthy operation, {DNNs} should be thoroughly tested. The {DeepFault} whitebox {DNN} testing approach presented in our paper addresses this challenge by employing suspiciousness measures inspired by fault localization to establish the hit spectrum of neurons and identify suspicious neurons whose weights have not been calibrated correctly and thus are considered responsible for inadequate {DNN} performance. {DeepFault} also uses a suspiciousness-guided algorithm to synthesize new inputs, from correctly classified inputs, that increase the activation values of suspicious neurons. Our empirical evaluation on several {DNN} instances trained on {MNIST} and {CIFAR}-10 datasets shows that {DeepFault} is effective in identifying suspicious neurons. Also, the inputs synthesized by {DeepFault} closely resemble the original inputs, exercise the identified suspicious neurons and are highly adversarial.},
	author = {Eniser, Hasan Ferit and Gerasimou, Simos and Sen, Alper},
	date = {2019},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Software Engineering (cs.{SE})},
}

@software{eniser_deepfault_2023,
	title = {{DeepFault}: Fault Localization for Deep Neural Networks},
	rights = {{GPL}-3.0},
	url = {https://github.com/hfeniser/DeepFault},
	shorttitle = {{DeepFault}},
	author = {Eniser, Hasan Ferit},
	urldate = {2023-10-08},
	date = {2023-07-31},
	note = {original-date: 2018-07-04T12:51:18Z},
}

@inproceedings{jones_empirical_2005,
	location = {Long Beach {CA} {USA}},
	title = {Empirical evaluation of the tarantula automatic fault-localization technique},
	isbn = {978-1-58113-993-8},
	doi = {10.1145/1101908.1101949},
	eventtitle = {{ASE}05: International Conference on Automated Software Engineering 2005},
	pages = {273--282},
	booktitle = {Proceedings of the 20th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Jones, James A. and Harrold, Mary Jean},
	date = {2005-11-07},
	langid = {english},
}

@article{ochiai_zoogeographical_1957,
	title = {Zoogeographical Studies on the Soleoid Fishes found in Japan and its Neighbouring Regions-I},
	volume = {22},
	issn = {1349-998X, 0021-5392},
	doi = {10.2331/suisan.22.522},
	pages = {522--525},
	number = {9},
	journaltitle = {{NIPPON} {SUISAN} {GAKKAISHI}},
	author = {Ochiai, Akira},
	urldate = {2023-10-09},
	date = {1957},
	langid = {english},
}

@article{wong_dstar_2014,
	title = {The {DStar} Method for Effective Software Fault Localization},
	volume = {63},
	issn = {0018-9529, 1558-1721},
	doi = {10.1109/TR.2013.2285319},
	pages = {290--308},
	number = {1},
	journaltitle = {{IEEE} Trans. Rel.},
	author = {Wong, W. Eric and Debroy, Vidroha and Gao, Ruizhi and Li, Yihao},
	date = {2014-03},
}

@article{wong_survey_2016,
	title = {A Survey on Software Fault Localization},
	volume = {42},
	issn = {0098-5589, 1939-3520},
	doi = {10.1109/TSE.2016.2521368},
	pages = {707--740},
	number = {8},
	journaltitle = {{IIEEE} Trans. Software Eng.},
	author = {Wong, W. Eric and Gao, Ruizhi and Li, Yihao and Abreu, Rui and Wotawa, Franz},
	date = {2016-08-01},
}

@article{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1708.07747},
	shorttitle = {Fashion-{MNIST}},
	abstract = {We present Fashion-{MNIST}, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-{MNIST} is intended to serve as a direct drop-in replacement for the original {MNIST} dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	urldate = {2023-10-10},
	date = {2017},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	date = {2015-05-28},
	langid = {english},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation Applied to Handwritten Zip Code Recognition},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	pages = {541--551},
	number = {4},
	journaltitle = {Neural Computation},
	author = {{LeCun}, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	date = {1989-12},
	langid = {english},
}

@misc{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	number = {{arXiv}:1412.6980},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2023-12-27},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{huang_survey_2020,
	title = {A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability},
	volume = {37},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2020.100270},
	shorttitle = {A survey of safety and trustworthiness of deep neural networks},
	pages = {100270},
	journaltitle = {Computer Science Review},
	author = {Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping},
	date = {2020-08},
	langid = {english},
}

@online{riebesell_convolution_2022,
	title = {Convolution Operator},
	url = {https://tikz.net/conv2d/},
	abstract = {Simple 2d example illustrating the role of the Jacobian determinant in the change of variables formula. Inspired by Ari Seff in https://youtu.be/i7LjDvsLWCg?t=250.},
	author = {Riebesell, Janosh},
	urldate = {2023-12-30},
	date = {2022-04-09},
	langid = {american},
	file = {Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\GM3QH9H3\\conv2d.html:text/html},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
	date = {2012},
}

@article{szegedy_going_2014,
	title = {Going Deeper with Convolutions},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC} 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC} 2014 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	date = {2014},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	doi = {10.1038/323533a0},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	date = {1986-10},
	langid = {english},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2023-12-29},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\WIH73JEL\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\CDKQIVSP\\1810.html:text/html},
}

@article{hendrycks_gaussian_2016,
	title = {Gaussian Error Linear Units ({GELUs})},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit ({GELU}), a high-performing neural network activation function. The {GELU} activation function is \${xΦ}(x)\$, where \$Φ(x)\$ the standard Gaussian cumulative distribution function. The {GELU} nonlinearity weights inputs by their value, rather than gates inputs by their sign as in {ReLUs} (\$x{\textbackslash}mathbf\{1\}\_\{x\&gt;0\}\$). We perform an empirical evaluation of the {GELU} nonlinearity against the {ReLU} and {ELU} activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	date = {2016},
	note = {Publisher: {arXiv}
Version Number: 5},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}

@inproceedings{glorot_deep_2011,
	location = {Fort Lauderdale, {FL}, {USA}},
	title = {Deep Sparse Rectifier Neural Networks},
	volume = {15},
	url = {https://proceedings.mlr.press/v15/glorot11a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
	pages = {315--323},
	booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	editor = {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
	date = {2011-04-11},
}

@article{fukushima_cognitron_1975,
	title = {Cognitron: A self-organizing multilayered neural network},
	volume = {20},
	issn = {0340-1200, 1432-0770},
	doi = {10.1007/BF00342633},
	shorttitle = {Cognitron},
	pages = {121--136},
	number = {3},
	journaltitle = {Biol. Cybernetics},
	author = {Fukushima, Kunihiko},
	date = {1975},
	langid = {english},
}

@inproceedings{glorot_understanding_2010,
	location = {Chia Laguna Resort, Sardinia, Italy},
	title = {Understanding the difficulty of training deep feedforward neural networks},
	volume = {9},
	url = {https://proceedings.mlr.press/v9/glorot10a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	pages = {249--256},
	booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	editor = {Teh, Yee Whye and Titterington, Mike},
	date = {2010-05-13},
}

@online{noauthor_tfkeraslayersdense_2023,
	title = {tf.keras.layers.Dense {\textbar} {TensorFlow} v2.14.0},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense},
	urldate = {2023-12-29},
	date = {2023-09-27},
	file = {tf.keras.layers.Dense  |  TensorFlow v2.14.0:C\:\\Users\\lenna\\Zotero\\storage\\CAWMREMW\\Dense.html:text/html},
}

@book{goodfellow_deep_2016,
	location = {Cambridge, Massachusetts},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	series = {Adaptive computation and machine learning},
	pagetotal = {775},
	publisher = {The {MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016},
	keywords = {Machine learning},
	file = {Goodfellow et al. - 2016 - Deep learning.pdf:C\:\\Users\\lenna\\Zotero\\storage\\MIZTQGIU\\Goodfellow et al. - 2016 - Deep learning.pdf:application/pdf},
}

@inproceedings{boureau_theoretical_2010,
	title = {A theoretical analysis of feature pooling in visual recognition},
	pages = {111--118},
	booktitle = {Proceedings of the 27th international conference on machine learning ({ICML}-10)},
	author = {Boureau, Y-Lan and Ponce, Jean and {LeCun}, Yann},
	date = {2010},
	keywords = {⛔ No {DOI} found},
	file = {Boureau et al. - 2010 - A theoretical analysis of feature pooling in visua.pdf:C\:\\Users\\lenna\\Zotero\\storage\\87LRD8XW\\Boureau et al. - 2010 - A theoretical analysis of feature pooling in visua.pdf:application/pdf},
}

@inproceedings{jain_supervised_2007,
	location = {Rio de Janeiro, Brazil},
	title = {Supervised Learning of Image Restoration with Convolutional Networks},
	isbn = {978-1-4244-1630-1},
	doi = {10.1109/ICCV.2007.4408909},
	eventtitle = {2007 {IEEE} 11th International Conference on Computer Vision},
	pages = {1--8},
	booktitle = {2007 {IEEE} 11th International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Jain, Viren and Murray, Joseph F. and Roth, Fabian and Turaga, Srinivas and Zhigulin, Valentin and Briggman, Kevin L. and Helmstaedter, Moritz N. and Denk, Winfried and Seung, H. Sebastian},
	date = {2007},
	file = {Jain et al. - 2007 - Supervised Learning of Image Restoration with Conv.pdf:C\:\\Users\\lenna\\Zotero\\storage\\PHT4F9BL\\Jain et al. - 2007 - Supervised Learning of Image Restoration with Conv.pdf:application/pdf},
}

@incollection{beyer_feature-guided_2018,
	location = {Cham},
	title = {Feature-Guided Black-Box Safety Testing of Deep Neural Networks},
	volume = {10805},
	isbn = {978-3-319-89959-6 978-3-319-89960-2},
	pages = {408--426},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	publisher = {Springer International Publishing},
	author = {Wicker, Matthew and Huang, Xiaowei and Kwiatkowska, Marta},
	editor = {Beyer, Dirk and Huisman, Marieke},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-89960-2_22},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Input Mutation, Safety Coverage},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\IG5JW6PX\\Wicker et al. - 2018 - Feature-Guided Black-Box Safety Testing of Deep Ne.pdf:application/pdf},
}

@inproceedings{sun_deepconcolic_2019,
	location = {Montreal, {QC}, Canada},
	title = {{DeepConcolic}: Testing and Debugging Deep Neural Networks},
	isbn = {978-1-72811-764-5},
	doi = {10.1109/ICSE-Companion.2019.00051},
	shorttitle = {{DeepConcolic}},
	eventtitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	pages = {111--114},
	booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering: Companion Proceedings ({ICSE}-Companion)},
	publisher = {{IEEE}},
	author = {Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Sharp, James and Hill, Matthew and Ashmore, Rob},
	date = {2019-05},
	keywords = {Symbolic Execution, Neuron Coverage},
}

@report{hayhurst_practical_2001,
	title = {A Practical Tutorial on Modified Condition/Decision Coverage},
	url = {https://ntrs.nasa.gov/citations/20010057789},
	abstract = {This tutorial provides a practical approach to assessing modified condition/decision coverage ({MC}/{DC}) for aviation software products that must comply with regulatory guidance for {DO}-178B level A software. The tutorial's approach to {MC}/{DC} is a 5-step process that allows a certification authority or verification analyst to evaluate {MC}/{DC} claims without the aid of a coverage tool. In addition to the {MC}/{DC} approach, the tutorial addresses factors to consider in selecting and qualifying a structural coverage analysis tool, tips for reviewing life cycle data related to {MC}/{DC}, and pitfalls common to structural coverage analysis.},
	number = {L-18088},
	author = {Hayhurst, Kelly J. and Veerhusen, Dan S. and Chilenski, John J. and Rierson, Leanna K.},
	urldate = {2024-01-04},
	date = {2001-05-01},
	note = {{NTRS} Author Affiliations: {NASA} Langley Research Center, Rockwell Collins, Inc., Boeing Co., Federal Aviation Administration
{NTRS} Document {ID}: 20010057789
{NTRS} Research Center: Langley Research Center ({LaRC})},
	keywords = {Computer Programming And Software},
	file = {Hayhurst et al. - 2001 - A Practical Tutorial on Modified ConditionDecisio.pdf:C\:\\Users\\lenna\\Zotero\\storage\\6CXKM9MN\\Hayhurst et al. - 2001 - A Practical Tutorial on Modified ConditionDecisio.pdf:application/pdf;Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\DZGLMH26\\20010057789.html:text/html},
}

@collection{wong_handbook_2023,
	location = {Hoboken, New Jersey, Piscataway, {NJ}},
	title = {Handbook of software fault localization: foundations and advances},
	isbn = {978-1-119-88092-9},
	shorttitle = {Handbook of software fault localization},
	abstract = {"Software fault localization is one of the most expensive activities in program debugging. It can be further divided into two major parts. The first part is to use a technique to identify suspicious code that may contain program bugs. The second part is for programmers to actually examine the identified code to decide whether it indeed contains bugs. Fault localization has historically been a manual task that has been recognized to be time consuming and tedious as well as prohibitively expensive, given the size and complexity of large-scale software systems today. Furthermore, manual fault localization relies heavily on the software developer's experience, judgment, and intuition to identify and prioritize code that is likely to be faulty. These limitations have led to a surge of interest in developing techniques that can partially or fully automate the localization of faults in software while reducing human input"-- Provided by publisher},
	publisher = {John Wiley \& Sons, Inc ; {IEEE} Press},
	editor = {Wong, W. Eric and Tse, T. H.},
	date = {2023},
	note = {{OCLC}: 1345466980},
	file = {Wong und Tse - 2023 - Handbook of software fault localization foundatio.pdf:C\:\\Users\\lenna\\Zotero\\storage\\WUGFAFA2\\Wong und Tse - 2023 - Handbook of software fault localization foundatio.pdf:application/pdf},
}

@book{parsa_software_2023,
	location = {Cham},
	title = {Software testing automation: testability evaluation, refactoring, test data generation and fault localization},
	isbn = {978-3-031-22057-9},
	shorttitle = {Software testing automation},
	abstract = {This book is about the design and development of tools for software testing. It intends to get the reader involved in software testing rather than simply memorizing the concepts. The source codes are downloadable from the book website. The book has three parts: software testability, fault localization, and test data generation. Part I describes unit and acceptance tests and proposes a new method called testability-driven development ({TsDD}) in support of {TDD} and {BDD}. {TsDD} uses a machine learning model to measure testability before and after refactoring. The reader will learn how to develop the testability prediction model and write software tools for automatic refactoring. Part {II} focuses on developing tools for automatic fault localization. This part shows the reader how to use a compiler generator to instrument source code, create control flow graphs, identify prime paths, and slice the source code. On top of these tools, a software tool, Diagnoser, is offered to facilitate experimenting with and developing new fault localization algorithms. Diagnoser takes a source code and its test suite as input and reports the coverage provided by the test cases and the suspiciousness score for each statement. Part {III} proposes using software testing as a prominent part of the cyber-physical system software to uncover and model unknown physical behaviors and the underlying physical rules. The reader will get insights into developing software tools to generate white box test data.},
	publisher = {Springer},
	author = {Parsa, Saeed},
	date = {2023},
	note = {{OCLC}: 1374251026},
	file = {Parsa - 2023 - Software testing automation testability evaluatio.pdf:C\:\\Users\\lenna\\Zotero\\storage\\7H47FSF9\\Parsa - 2023 - Software testing automation testability evaluatio.pdf:application/pdf},
}

@inproceedings{hua_jie_lee_study_2009,
	location = {Beijing, China},
	title = {Study of the relationship of bug consistency with respect to performance of spectra metrics},
	isbn = {978-1-4244-4519-6},
	doi = {10.1109/ICCSIT.2009.5234512},
	eventtitle = {2009 2nd {IEEE} International Conference on Computer Science and Information Technology},
	pages = {501--508},
	booktitle = {2009 2nd {IEEE} International Conference on Computer Science and Information Technology},
	publisher = {{IEEE}},
	author = {{Hua Jie Lee} and Naish, Lee and {Kotagiri Ramamohanarao}},
	date = {2009},
	keywords = {Trantula metric symplified.},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\2Q6V9238\\Hua Jie Lee et al. - 2009 - Study of the relationship of bug consistency with .pdf:application/pdf},
}

@article{yoo_human_2017,
	title = {Human Competitiveness of Genetic Programming in Spectrum-Based Fault Localisation: Theoretical and Empirical Analysis},
	volume = {26},
	issn = {1049-331X, 1557-7392},
	doi = {10.1145/3078840},
	shorttitle = {Human Competitiveness of Genetic Programming in Spectrum-Based Fault Localisation},
	abstract = {We report on the application of Genetic Programming to Software Fault Localisation, a problem in the area of Search-Based Software Engineering ({SBSE}). We give both empirical and theoretical evidence for the human competitiveness of the evolved fault localisation formulæ under the single fault scenario, compared to those generated by human ingenuity and reported in many papers, published over more than a decade. Though there have been previous human competitive results claimed for {SBSE} problems, this is the first time that evolved solutions have been formally proved to be human competitive. We further prove that no future human investigation could outperform the evolved solutions. We complement these proofs with an empirical analysis of both human and evolved solutions, which indicates that the evolved solutions are not only theoretically human competitive, but also convey similar practical benefits to human-evolved counterparts.},
	pages = {1--30},
	number = {1},
	journaltitle = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Yoo, Shin and Xie, Xiaoyuan and Kuo, Fei-Ching and Chen, Tsong Yueh and Harman, Mark},
	date = {2017-01-31},
	langid = {english},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\SCKXPB3H\\Yoo et al. - 2017 - Human Competitiveness of Genetic Programming in Sp.pdf:application/pdf},
}

@online{noauthor_build_2023,
	title = {Build from source on Windows {\textbar} {TensorFlow}},
	url = {https://www.tensorflow.org/install/source_windows},
	urldate = {2024-01-11},
	date = {2023-11-21},
	langid = {english},
	file = {Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\FGTTEVX3\\source_windows.html:text/html},
}

@misc{martin_abadi_tensorflow_2015,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	date = {2015},
}

@article{sohn_arachne_2023,
	title = {Arachne: Search-Based Repair of Deep Neural Networks},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	doi = {10.1145/3563210},
	shorttitle = {Arachne},
	abstract = {The rapid and widespread adoption of Deep Neural Networks ({DNNs}) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of {DNNs}. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This article introduces Arachne, a novel program repair technique for {DNNs}, which directly repairs {DNNs} using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses differential evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a {DNN} without reducing general accuracy significantly. On average, patches generated by Arachne generalise to
              61.3\%
              of unseen misbehaviour, whereas those by a state-of-the-art {DNN} repair technique generalise only to
              10.2\% and sometimes to none while taking tens of times more than Arachne.
              We also show that Arachne can address fairness issues by debiasing a gender classification model.
              Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond convolutional neural networks.},
	pages = {1--26},
	number = {4},
	journaltitle = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Sohn, Jeongju and Kang, Sungmin and Yoo, Shin},
	date = {2023-10-31},
	langid = {english},
	keywords = {Model centric Repair},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\6KGVTB6A\\Sohn et al. - 2023 - Arachne Search-Based Repair of Deep Neural Networ.pdf:application/pdf},
}

@inproceedings{zhang_apricot_2019,
	location = {San Diego, {CA}, {USA}},
	title = {Apricot: A Weight-Adaptation Approach to Fixing Deep Learning Models},
	isbn = {978-1-72812-508-4},
	doi = {10.1109/ASE.2019.00043},
	shorttitle = {Apricot},
	eventtitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {376--387},
	booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Zhang, Hao and Chan, W.K.},
	date = {2019-11},
	keywords = {Model centric Repair},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\UBIY8MVK\\Zhang und Chan - 2019 - Apricot A Weight-Adaptation Approach to Fixing De.pdf:application/pdf},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	pages = {2825--2830},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	date = {2011},
}

@software{collette_h5pyh5py_2022,
	title = {h5py/h5py: 3.7.0},
	rights = {Open Access},
	shorttitle = {h5py/h5py},
	abstract = {{HDF}5 for Python -- The h5py package is a Pythonic interface to the {HDF}5 binary data format.},
	version = {3.7.0},
	publisher = {Zenodo},
	author = {Collette, Andrew and Kluyver, Thomas and Caswell, Thomas A and Tocknell, James and Kieffer, Jerome and Jelenak, Aleksandar and Scopatz, Anthony and Dale, Darren and Chen and VINCENT, Thomas and Einhorn, Matt and {Payno} and {Juliagarriga} and {Pierlauro Sciarelli} and Valls, Valentin and {Satrajit Ghosh} and Pedersen, Ulrik Kofoed and {Jakirkham} and Raspaud, Martin and Danilevski, Cyril and {Hameer Abbasi} and Readey, John and Mühlbauer, Kai and Paramonov, Andrey and Chan, Lawrence and {V. Armando Solé} and {Jialin} and Guest, Daniel Hay and Feng, Yu and Kittisopikul, Mark},
	date = {2022-05-24},
	doi = {10.5281/ZENODO.6575970},
}

@inproceedings{pei_deepxplore_2017,
	title = {{DeepXplore}: Automated Whitebox Testing of Deep Learning Systems},
	doi = {10.1145/3132747.3132785},
	shorttitle = {{DeepXplore}},
	abstract = {Deep learning ({DL}) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing {DL} testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs. We design, implement, and evaluate {DeepXplore}, the first whitebox framework for systematically testing real-world {DL} systems. First, we introduce neuron coverage for systematically measuring the parts of a {DL} system exercised by test inputs. Next, we leverage multiple {DL} systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for {DL} systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques. {DeepXplore} efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art {DL} models with thousands of neurons trained on five popular datasets including {ImageNet} and Udacity self-driving challenge data. For all tested {DL} models, on average, {DeepXplore} generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by {DeepXplore} can also be used to retrain the corresponding {DL} model to improve the model's accuracy by up to 3\%.},
	pages = {1--18},
	booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
	author = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
	date = {2017-10-14},
	eprinttype = {arxiv},
	eprint = {1705.06640 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
}

@inproceedings{cheng_manifesting_2018,
	location = {Lisbon},
	title = {Manifesting Bugs in Machine Learning Code: An Explorative Study with Mutation Testing},
	isbn = {978-1-5386-7757-5},
	doi = {10.1109/QRS.2018.00044},
	shorttitle = {Manifesting Bugs in Machine Learning Code},
	eventtitle = {2018 {IEEE} International Conference on Software Quality, Reliability and Security ({QRS})},
	pages = {313--324},
	booktitle = {2018 {IEEE} International Conference on Software Quality, Reliability and Security ({QRS})},
	publisher = {{IEEE}},
	author = {Cheng, Dawei and Cao, Chun and Xu, Chang and Ma, Xiaoxing},
	date = {2018-07},
}

@software{chollet_keras_2015,
	title = {Keras},
	url = {https://keras.io},
	author = {Chollet, François and {others}},
	date = {2015},
}

@article{yu_deeprepair_2022,
	title = {\textit{{DeepRepair}:} Style-Guided Repairing for Deep Neural Networks in the Real-World Operational Environment},
	volume = {71},
	issn = {0018-9529, 1558-1721},
	doi = {10.1109/TR.2021.3096332},
	shorttitle = {{\textless}i{\textgreater}{DeepRepair}},
	pages = {1401--1416},
	number = {4},
	journaltitle = {{IEEE} Trans. Rel.},
	author = {Yu, Bing and Qi, Hua and Guo, Qing and Juefei-Xu, Felix and Xie, Xiaofei and Ma, Lei and Zhao, Jianjun},
	date = {2022-12},
	keywords = {Data centric},
}

@inproceedings{nakagawa_experience_2023,
	location = {Taipa, Macao},
	title = {An Experience Report on Regression-Free Repair of Deep Neural Network Model},
	isbn = {978-1-66545-278-6},
	doi = {10.1109/SANER56733.2023.00090},
	eventtitle = {2023 {IEEE} International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	pages = {778--782},
	booktitle = {2023 {IEEE} International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	publisher = {{IEEE}},
	author = {Nakagawa, Takao and Tokumoto, Susumu and Tokui, Shogo and Ishikawa, Fuyuki},
	date = {2023-03},
	keywords = {Model centric Repair},
}

@inproceedings{li_calsi_adaptive_2023,
	location = {Lisbon Portugal},
	title = {Adaptive Search-based Repair of Deep Neural Networks},
	isbn = {9798400701191},
	doi = {10.1145/3583131.3590477},
	eventtitle = {{GECCO} '23: Genetic and Evolutionary Computation Conference},
	pages = {1527--1536},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	publisher = {{ACM}},
	author = {Li Calsi, Davide and Duran, Matias and Laurent, Thomas and Zhang, Xiao-Yi and Arcaini, Paolo and Ishikawa, Fuyuki},
	date = {2023-07-15},
	langid = {english},
	keywords = {Model centric Repair},
}

@inproceedings{calsi_distributed_2023,
	title = {Distributed Repair of Deep Neural Networks},
	doi = {10.1109/ICST57152.2023.00017},
	pages = {83--94},
	booktitle = {2023 {IEEE} Conference on Software Testing, Verification and Validation ({ICST})},
	author = {Calsi, Davide Li and Duran, Matias and Zhang, Xiao-Yi and Arcaini, Paolo and Ishikawa, Fuyuki},
	date = {2023},
	keywords = {Deep learning, Neural networks, automated repair, Collaboration, Computational modeling, {DNNs}, risk levels, Scalability, Software algorithms, Software testing, Model centric Repair},
	file = {Calsi et al. - 2023 - Distributed Repair of Deep Neural Networks.pdf:C\:\\Users\\lenna\\Zotero\\storage\\YN8QXA4Q\\Calsi et al. - 2023 - Distributed Repair of Deep Neural Networks.pdf:application/pdf},
}

@misc{tokui_neurecover_2022,
	title = {{NeuRecover}: Regression-Controlled Repair of Deep Neural Networks with Training History},
	url = {http://arxiv.org/abs/2203.00191},
	shorttitle = {{NeuRecover}},
	abstract = {Systematic techniques to improve quality of deep neural networks ({DNNs}) are critical given the increasing demand for practical applications including safety-critical ones. The key challenge comes from the little controllability in updating {DNNs}. Retraining to fix some behavior often has a destructive impact on other behavior, causing regressions, i.e., the updated {DNN} fails with inputs correctly handled by the original one. This problem is crucial when engineers are required to investigate failures in intensive assurance activities for safety or trust. Search-based repair techniques for {DNNs} have potentials to tackle this challenge by enabling localized updates only on "responsible parameters" inside the {DNN}. However, the potentials have not been explored to realize sufficient controllability to suppress regressions in {DNN} repair tasks. In this paper, we propose a novel {DNN} repair method that makes use of the training history for judging which {DNN} parameters should be changed or not to suppress regressions. We implemented the method into a tool called {NeuRecover} and evaluated it with three datasets. Our method outperformed the existing method by achieving often less than a quarter, even a tenth in some cases, number of regressions. Our method is especially effective when the repair requirements are tight to fix specific failure types. In such cases, our method showed stably low rates ({\textless}2\%) of regressions, which were in many cases a tenth of regressions caused by retraining.},
	number = {{arXiv}:2203.00191},
	publisher = {{arXiv}},
	author = {Tokui, Shogo and Tokumoto, Susumu and Yoshii, Akihito and Ishikawa, Fuyuki and Nakagawa, Takao and Munakata, Kazuki and Kikuchi, Shinji},
	urldate = {2024-02-25},
	date = {2022-03-04},
	eprinttype = {arxiv},
	eprint = {2203.00191 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Model centric Repair},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\YGUQ2587\\Tokui et al. - 2022 - NeuRecover Regression-Controlled Repair of Deep N.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\FG47AZZD\\2203.html:text/html},
}

@inproceedings{zhang_autotrainer_2021,
	location = {Madrid, {ES}},
	title = {{AUTOTRAINER}: An Automatic {DNN} Training Problem Detection and Repair System},
	isbn = {978-1-66540-296-5},
	doi = {10.1109/ICSE43902.2021.00043},
	shorttitle = {{AUTOTRAINER}},
	eventtitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering ({ICSE})},
	pages = {359--371},
	booktitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering ({ICSE})},
	publisher = {{IEEE}},
	author = {Zhang, Xiaoyu and Zhai, Juan and Ma, Shiqing and Shen, Chao},
	date = {2021-05},
	keywords = {Training centric},
	file = {Zhang et al. - 2021 - AUTOTRAINER An Automatic DNN Training Problem Det.pdf:C\:\\Users\\lenna\\Zotero\\storage\\ANLAHBXG\\Zhang et al. - 2021 - AUTOTRAINER An Automatic DNN Training Problem Det.pdf:application/pdf},
}

@misc{wardat_deepdiagnosis_2021,
	title = {{DeepDiagnosis}: Automatically Diagnosing Faults and Recommending Actionable Fixes in Deep Learning Programs},
	url = {http://arxiv.org/abs/2112.04036},
	shorttitle = {{DeepDiagnosis}},
	abstract = {Deep Neural Networks ({DNNs}) are used in a wide variety of applications. However, as in any software application, {DNN}-based apps are afflicted with bugs. Previous work observed that {DNN} bug fix patterns are different from traditional bug fix patterns. Furthermore, those buggy models are non-trivial to diagnose and fix due to inexplicit errors with several options to fix them. To support developers in locating and fixing bugs, we propose {DeepDiagnosis}, a novel debugging approach that localizes the faults, reports error symptoms and suggests fixes for {DNN} programs. In the first phase, our technique monitors a training model, periodically checking for eight types of error conditions. Then, in case of problems, it reports messages containing sufficient information to perform actionable repairs to the model. In the evaluation, we thoroughly examine 444 models -53 real-world from {GitHub} and Stack Overflow, and 391 curated by {AUTOTRAINER}. {DeepDiagnosis} provides superior accuracy when compared to {UMLUAT} and {DeepLocalize}. Our technique is faster than {AUTOTRAINER} for fault localization. The results show that our approach can support additional types of models, while state-of-the-art was only able to handle classification ones. Our technique was able to report bugs that do not manifest as numerical errors during training. Also, it can provide actionable insights for fix whereas {DeepLocalize} can only report faults that lead to numerical errors during training. {DeepDiagnosis} manifests the best capabilities of fault detection, bug localization, and symptoms identification when compared to other approaches.},
	number = {{arXiv}:2112.04036},
	publisher = {{arXiv}},
	author = {Wardat, Mohammad and Cruz, Breno Dantas and Le, Wei and Rajan, Hridesh},
	urldate = {2024-02-26},
	date = {2021-12-07},
	eprinttype = {arxiv},
	eprint = {2112.04036 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Training centric},
}

@inproceedings{gao_fuzz_2020,
	location = {Seoul South Korea},
	title = {Fuzz testing based data augmentation to improve robustness of deep neural networks},
	isbn = {978-1-4503-7121-6},
	doi = {10.1145/3377811.3380415},
	eventtitle = {{ICSE} '20: 42nd International Conference on Software Engineering},
	pages = {1147--1158},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Gao, Xiang and Saha, Ripon K. and Prasad, Mukul R. and Roychoudhury, Abhik},
	date = {2020-06-27},
	langid = {english},
	keywords = {Data centric},
}

@misc{ciresan_multi-column_2012,
	title = {Multi-column Deep Neural Networks for Image Classification},
	url = {http://arxiv.org/abs/1202.2745},
	abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive {MNIST} handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
	number = {{arXiv}:1202.2745},
	publisher = {{arXiv}},
	author = {Cireşan, Dan and Meier, Ueli and Schmidhuber, Juergen},
	urldate = {2024-02-28},
	date = {2012-02-13},
	eprinttype = {arxiv},
	eprint = {1202.2745 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Image Recognition},
}

@article{jiang_exploiting_2018,
	title = {Exploiting Feature and Class Relationships in Video Categorization with Regularized Deep Neural Networks},
	volume = {40},
	issn = {0162-8828, 2160-9292},
	doi = {10.1109/TPAMI.2017.2670560},
	pages = {352--364},
	number = {2},
	journaltitle = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Jiang, Yu-Gang and Wu, Zuxuan and Wang, Jun and Xue, Xiangyang and Chang, Shih-Fu},
	date = {2018-02-01},
	keywords = {Video Recognition},
}

@article{hinton_deep_2012,
	title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
	volume = {29},
	issn = {1053-5888},
	doi = {10.1109/MSP.2012.2205597},
	shorttitle = {Deep Neural Networks for Acoustic Modeling in Speech Recognition},
	pages = {82--97},
	number = {6},
	journaltitle = {{IEEE} Signal Process. Mag.},
	author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
	date = {2012-11},
	file = {Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf:C\:\\Users\\lenna\\Zotero\\storage\\AAB85WD7\\Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf:application/pdf},
}

@article{litjens_survey_2017,
	title = {A survey on deep learning in medical image analysis},
	volume = {42},
	issn = {13618415},
	doi = {10.1016/j.media.2017.07.005},
	pages = {60--88},
	journaltitle = {Medical Image Analysis},
	author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and Van Der Laak, Jeroen A.W.M. and Van Ginneken, Bram and Sánchez, Clara I.},
	date = {2017-12},
	langid = {english},
	keywords = {Medical Image Analysis},
	file = {Volltext:C\:\\Users\\lenna\\Zotero\\storage\\SHYDWGBD\\Litjens et al. - 2017 - A survey on deep learning in medical image analysi.pdf:application/pdf},
}

@misc{bojarski_end_2016,
	title = {End to End Learning for Self-Driving Cars},
	url = {http://arxiv.org/abs/1604.07316},
	abstract = {We trained a convolutional neural network ({CNN}) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an {NVIDIA} {DevBox} and Torch 7 for training and an {NVIDIA} {DRIVE}({TM}) {PX} self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second ({FPS}).},
	number = {{arXiv}:1604.07316},
	publisher = {{arXiv}},
	author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
	urldate = {2024-02-28},
	date = {2016-04-25},
	eprinttype = {arxiv},
	eprint = {1604.07316 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\lenna\\Zotero\\storage\\62QFLGGY\\Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\5WKM92WE\\1604.html:text/html},
}

@online{keras_release_nodate,
	title = {Release Keras 2.3.0 · keras-team/keras},
	url = {https://github.com/keras-team/keras/releases/tag/2.3.0},
	abstract = {Keras 2.3.0 is the first release of multi-backend Keras that supports {TensorFlow} 2.0. It maintains compatibility with {TensorFlow} 1.14, 1.13, as well as Theano and {CNTK}.
This release brings the {API} ...},
	titleaddon = {{GitHub}},
	urldate = {2024-03-01},
	langid = {english},
	file = {Snapshot:C\:\\Users\\lenna\\Zotero\\storage\\DJI3X4XT\\2.3.html:text/html},
}