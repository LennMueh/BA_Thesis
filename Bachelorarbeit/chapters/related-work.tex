In this chapter, we will discuss proposed approaches for repairing neural networks.
These approaches \cite{nakagawa_experience_2023} can be classified into three categories: training-centric, data-centric, and model-centric.

\section{Training-Centric Approaches}\label{sec:training-centric-approaches}


\section{Data-Centric Approaches}\label{sec:data-centric-approaches}
One data centric approach for the repair of deep neural networks is DeepRepair \cite{yu_deeprepair_2022} which uses a style guided approach to enhance the training data of an DNN. The Idea for this approach is it to mitigate the gap between the training data and the real-world data provided later when the network is in production, which often contains noise patterns.
This is done by the style of guided data augmentation by introducing the noise patterns which are frequently leading to failure.

To make the augmentation more effective, DeepRepair uses a clustering-based method for generating corrupted data, by identifying and grouping similar failure patterns.
There by, it is ensured that the data covers a broaderspectrum of potential real-world failure scenarios, enhancing the robustness of the model.

Another data-centric approach is SENSEI \cite{gao_fuzz_2020}, which uses a fuzz testing derived data augmentation approach to bridge the earlier described gap by exposing and mitigating vulnerabilities in DNNs.

\section{Model-Centric Approaches}\label{sec:model-centric-approaches}


\section{DeepFault}\label{sec:deepfault}
DeepFault\cite{eniser_deepfault_2019} is a white-box testing approach for neural networks, developed by Eniser et al. which is according to the Authors.
\begin{quote}
    ... the first fault localization-based white-box testing approach for DNNs.
\end{quote}
There are two objectives to the approach, the identification of suspicious neurons, which have undesirable behaviour, where the respective neuron is suspected to lead to an undesirable outcome in the whole network.
And the synthesis of new inputs for the neural network, to specially retrain the (most) suspicious values.

In this First Part, DeepFault is establishing a Hit Spectrum($HS$) \ref{eq:hit_spectrum} for all neurons, which take the form of a tuple.
The input, and output layers are left out because they are considered inherently correct.
\begin{equation}
    HS_n = (attr_n^{as}, attr_n^{af}, attr_n^{ns}, attr_n^{nf})\label{eq:hit_spectrum}
\end{equation}
\begin{itemize}
    \item $attr^{as}_n$ is the number of times the neuron $n$ is activated in successful test cases.
    \item $attr^{af}_n$ is the number of times the neuron $n$ is activated in failed test cases.
    \item $attr^{ns}_n$ is the number of times the neuron $n$ is not activated in successful test cases.
    \item $attr^{nf}_n$ is the number of times the neuron $n$ is not activated in failed test cases.
\end{itemize}
Then the hit spectra are in a suspiciousness measures, in DeepFault Tarantula \ref{eq: tarantula}, Ochiai \ref{eq: ochiai}, $D^3$ \ref{eq: dstar} are used.
These are not used to identify one wrong neuron, but rather a set of wrong neurons.
For that, the suspiciousness values of the neurons are used to sort the neurons in decreasing order of suspiciousness, if multiple neurons happen to get the same value, the neuron in a deeper layer is used.

Guided by the suspiciousness measures, DeepFault modifies the input, for which the neural network has made the correct decision, in a targeted way.
The synthesis task is supported by a gradient ascent algorithm that aims to determine the degree to which an appropriately classified input should and could be modified to enhance the activation values of suspicious neurons.