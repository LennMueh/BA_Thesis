In the previous chapters, we introduced a new method for repairing Deep Neural Networks and Convolutional Neural Networks.
This method uses a spectrum-based fault localization combined with network mutations to increase network accuracy and decrease network loss.
First, we analyse the network to identify any faults.
We then use these findings to apply our mutation function.
If desired, we can train the network for an additional epoch.
Finally, we evaluate the network again.
If its performance has declined, we revert to the previous network.
If its performance has improved, we keep the new network and repeat the process.

During the first three epochs of our approach, we observed an increase in accuracy of up to 2--5\% for the first epoch and up to 11--25\% for untrained models.
The first epoch showed a decrease of up to 10--40\%, and the third epoch showed a decrease of up to 25--40\% compared to our benchmark.
However, there was no further increase in accuracy for subsequent epochs, only a decrease.

If we further trained our models, we get an increase of up to 2.5-7.5\% for the accuracy and a decrease of up to 12--35\% for the loss.
Here we also see a decrease after the third epoch, but we still stay over the baseline of one for the first 10â€“11 epochs.

The approach performs best on the full Fashion-MNIST dataset and also good on the half data set.
For our suspiciousness measures which we utilize for our fault localization, tarantula seems to be the best performing measure, for the trained models, the other measures aren't working significantly better than the random choosing of neurons.
The exception is in the accuracy category for the non-trained models, the ochiai measure.
Our Approach seems to be working best on Convolutional Neural Networks, in contrast, we only see a marginal increase for Deep Neural Networks.
For our break conditions, we see no difference in the trained conditions for the trained runs, but we see one for the non-trained ones, where we see the worst performance which depends more on the loss, this holds even true for the loss.
Important is also the use of an Offset for both loss and accuracy.
Another factor that can be mitigated is the use of the different mutation functions, where we see that the mutation functions aren't leading to significantly different results, with the exception that the weight mutation function are leading to better results than the bias targeting mutation functions.

In future research, it would be beneficial to investigate several aspects.
Firstly, it is essential to evaluate the performance of the approach on larger models and different datasets.
This will help to determine a threshold beyond which further modification of the neural network is not possible.
For instance, this threshold could be defined as a ratio between the model's modifiable parameters and the modified parameters.
Another thing that could be investigated in this instance is the choosing of multiple neurons at a time for the modification.

Due to time constraints, we decided to use the static spectrum matrix for fault localization instead of investigating other critical options.
A faster approach for generating the matrix should be considered.
This would lead to the possibility to change a neuron, until another more suspicious neuron arises.

One element which we also introduced due to the time constraints of this thesis is the regression for the loss and accuracy offset, to get a faster release of the break condition, hence getting runs with fewer epochs.
So the behaviour without the offset would be interesting.

Another possible area of investigation would be the suspiciousness measures, one point would be the usage of suspiciousness measures based on genetic programming which superiority in spectrum-based fault localisation regarding software was already shown by Yoo et al. \cite{yoo_human_2017} which could increase the right choosing of the neurons.
Another endeavour worth exploring would be the dstar suspiciousness measure, which we used in combination with the star value of 3, hence the investigation of values different to 3 would be needed.

A different idea would be to use the spectrum-based fault localization to not only identify suspicious neurons, but to aggregate the suspiciousness of a layer and then to modify these.
For example, we could change the activation function of the layer, the stride of a convolutional layer or pooling layer, the kernel/filter of a convolutional layer, or we could change the size of the pooling layer.
An approach which should be tried is the freezing of an overly trustful layer could be frozen to focus on the training of the other layers.

Or another appropriate layer could be set as a predecessor or successor.
For example, a normalization layer, a dense layer or a dropout layer as a predecessor or a regularization layer as a successor.
Another idea would be the encapsulation of the layer in a residual block.
At last, we could add an attention layer on either side of the suspicious layer.

Another useful feature would be the automatically choosing of an appropriate fix when a condition is met in the suspicious neuron, like an exploding or vanishing gradient for example.
Finally, we could modify the activation function of the specified neuron using a custom layer that applies the desired activation function to each neuron, to address things like dying ReLU.