In this chapter, we will discuss the performance of our approach towards neural networks.
We will explore various options available through our approach to elicit optimal results.
To achieve this, the evaluation will be based on the following research questions:
\begin{enumerate}
    \item[]\textbf{RQ1: Impact of Training on Mutated Models} How does training a mutated pre-trained model impact its performance metrics?
    \item[]\textbf{RQ2: Effects of Mutation Without Training} What is the effect on performance metrics when a pre-trained model is mutated without further training?
    \item[]\textbf{RQ3: Effects of the Extent of pre-Training} What is the effect on performance metrics, depending on how many epochs the model is pre-trained?
    \item[]\textbf{RQ4: Training Dataset Size and Approach Performance} What is the impact of the size of the training dataset on the approach's effectiveness?
    \item[]\textbf{RQ5: Influence of Suspiciousness Measures} How do different suspiciousness measures influence the outcomes of the experiments?
    \item[]\textbf{RQ6: CNN vs. DNN Architectural Efficiency} Which architecture yields better results for our approach: CNN or DNN?
    \item[]\textbf{RQ7: Offset Variations in Loss and Accuracy} How do variations in offset for loss and accuracy affect the approach's performance?
    \item[]\textbf{RQ8: Break Conditions and Algorithm Performance} What is the effect of different break conditions on the efficiency and effectiveness of the algorithm?
    \item[]\textbf{RQ9: Contributions of Different Mutation Functions} How do different mutation functions contribute to the model's performance with our Algorithm?
\end{enumerate}
\section{Setup}\label{sec:setup}

We evaluated our approach on a Workstation consisting of an AMD Ryzen 9--3900X 12-Core Processor 4,6 GHz, with 32 GB of RAM and an NVIDIA GeForce RTX 4070Ti GPU with 12 GB of VRAM and 7680 CUDA Cores.
The setup is running Ubuntu release 22.04, running with Windows Subsystem for Linux, on Microsoft Windows 11 Pro, we use it because since version 2.11\cite{noauthor_build_2023} TensorFlow doesn't support GPU acceleration natively any more.
Regarding the Software, we are using Python version 3.10.12 and using TensorFlow version 2.14.1, with CUDA version 12.3.

\subsection{Architecture}\label{subsec:architecture}
We are using the Fashion-MNIST dataset\cite{xiao_fashion-mnist_2017} for our experiments, which consists of 60,000 training images and 10,000 test images, each of size 28Ã—28 pixels, with 10 classes.
For the evaluation we haven't used not only the whole, but also a half and a quarter of the training data, which are derived from the original training data, by using Scikit-learn's\cite{pedregosa_scikit-learn_2011} \texttt{train\_test\_split} function, with a test size of 0.5 and 0.75 respectively.

We used 2 DNNs and 2 CNNs, in the following table \ref{tab:archi} a plain number describes the number of neurons in a dense layer. $CL$ describes a combination of a convolutional layer with a stride of $(1,1)$ and a kernel size of $(3,3)$ and a pooling layer with a pool size of $(2,2)$. We used.
Adam as our optimizer with a learning rate of 0.001.

The models were selected for their small size and ability to accommodate both deep neural networks and convolutional neural networks.
This was done to save time and allow for the evaluation of as many parameters as possible within the time frame of this thesis.
\input{tables/architecture_table.tex}
\subsection{Parameters}\label{subsec:parameters}
Fo our evaluation we used the following parameters:
\begin{enumerate}
    \item[]\textbf{Similarity coefficient:} tarantula, dstar with value 3, ochiai and random
    \item[]\textbf{Mutation functions:} \texttt{modify\_weight\_one\_random\_gauss,\\ modify\_weight\_all\_random\_gauss, modify\_bias, modify\_bias\_random\_gauss, modify\_all\_weights, modify\_all\_weights\_by\_scalar,\\modify\_all\_weights\_by\_scalar\_random\_gauss,\\modify\_weight\_all\_random\_by\_scalar\_gauss}
    \item[]\textbf{Break conditions:} loss, accuracy, loss and accuracy, loss or accuracy
    \item[]\textbf{Loss offset:} 0.005, 0
    \item[]\textbf{Accuracy offset:} 0.01, 0
    \item[]\textbf{Loss and accuracy regression:} True for all runs
    \item[]\textbf{Values:} -1, -0.5, 0, 0.5, 1 \textit{0 just for value assignment, basically a deletation of a neuron}
    \item[]\textbf{Sigma for random:} 0.5, 1
\end{enumerate}
\section{Results}\label{sec:results}
\input{chapters/results1}
