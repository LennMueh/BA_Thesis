\section{Neural Networks}\label{sec:neural-networks}
Without further ado, we will discuss what neural networks are and how they are composed.
We describe what a layer is and what types there are.
For what do we need optimizers and activation functions and what types are there.
Neural Networks are useful in a multitude of fields like classification, regression, transcription, or machine translation. \cite{goodfellow_deep_2016}
Neuronal networks are composed of layers, which are composed of neurons.
The neuron is the computational component in a deep neural network.\\
A neuron transforms a vector of inputs $\mathbf{x}$ to the output $y$, $\mathbb{R}^n \to \mathbb{R}$
by using a vector of weights $\mathbf{w}$, a bias $b$ and an activation function $f$ \ref{eq: NeuralFunction}.
\begin{equation}
    y = f\left( \sum^n_{i=1} w_i\cdot x_i + b\right)
    \label{eq: NeuralFunction}
\end{equation}
For Example, in the machine learning software library TensorFlow, which is based on Keras, uses by default Glorot uniform initializer\cite{noauthor_tfkeraslayersdense_2023,glorot_understanding_2010},
also called Xavier uniform initializer for the weights and initializes the bias with zeros.\\
Another important component, are the activation functions, one of the most common ones is the sigmoid function, which is today mostly used in output layers.
The rectified linear unit, short ReLU \ref{eq: relu} which was first used 1975 by Fukushima\cite{fukushima_cognitron_1975} and shown in 2011 by Glorot et al. \cite{glorot_deep_2011}
that it enables a better training for a neural network compared to the till then common sigmoid function.
\begin{equation}
    f(x) =
    \begin{cases}
        x& x > 0\\
        0& x <= 0
    \end{cases}
    \label{eq: relu}
\end{equation}
Another even more promising activation function is the Gaussian Error Linear Unit, short GELU \ref{eq: relu} which was proposed by Hendrycks and Gimpel in 2016\cite{hendrycks_gaussian_2016} is used by natural language processing models like BERT\cite{devlin_bert_2019}.
It does not just set all negative values to zero like ReLU but weights them.
The differences can be seen in the figure. \ref{fig: activation function}
\begin{equation}
    f(x) = 0.5x\left ( 1+\tanh\left [ \sqrt{\frac{2}{\pi}}\left ( x+0.044715x^3 \right ) \right ] \right )
    \label{eq: gelu}
\end{equation}
\input{figures/activationfunction.tikz}
The neurons are then combined into layers, in a Deep Neural Network (DNN) we have three types of Layers:
Input layers, output layers and the hidden layers.
In general are there is only one input and one output layer \ref{fig: neural network}, but there is a plurality of hidden layers.
The input layer, is pretty straightforward, it processes the input for the hidden layers, for example, it converts a 2-dimensional picture to a 1-dimensonal array,
which can then be processed by the subsequent layers.
The hidden layer does most of the heavy lifting, it performs most of the computational work of a DNN, it works based on the formula from above.
The output layer, takes the output of the last hidden layer, transforms it so that we get a meaningful output.
For a regression or binary classification, we have mostly just one neuron, but for a multi-class classification the number of neurons corresponds with the number of classes that shall be classified.
The output layer also works after the same principle of the formula above, but not a “normal” activation function is used.
Either a no activation function is used or a linear activation function is used for a regression or the softmax \ref{eq: softmax} function for a classification model.
\begin{equation}
    f(x_i) = \frac{e^{x_i}}{\sum^n_{j=1}e^{x_j}}
    \label{eq: softmax}
\end{equation}
\input{figures/neuralnetwork.tikz}
But, how are deep neural networks trained? \cite{lecun_deep_2015}
The first step is forward propagation, the training data is fed in to the network and flows through the layers, where each neuron performs it operation. \ref{eq: NeuralFunction}
Afterward the backpropagation or backpropagation of error is performed. \cite{rumelhart_learning_1986}
Then the output is compared to the desired outcome of the training data, hence, if the predicted outcome of the network matches the actual outcome.
If it doesn't match, it calculates how far off the prediction is.
The process of backpropagation is initiated, which uses an algorithm like Stochastic Gradient Descent (SGD) or one of its modern successors like Adam.
SGD updates the model parameters by using the gradient of the loss function concerning the parameter.
This isn't done for the whole dataset but just some data points.
The learning rate describes how far a parameter is changed in the direction of a derivation for one iteration, in SGD this Learning Rate is constant and manually set.

Adaptive Moment Estimation (Adam)\cite{kingma_adam_2017} is one of the successors of SGD. Adam, uses a variable learning rate which is adapted for each parameter, which uses the estimation of the first moment, the mean, and the second moment, the variance to change the learning rate.

Then this process is repeated until we are gone through the entire training set, which is called an epoch.
\subsection{Core Layers in Neural Networks}\label{subsec:core-layers-in-neural-networks}
\subsubsection{Dense Layers}
Dense Layers are the backbone of neural networks.
They do most of the computational work of a neural network.
It's characterized by its fully connected architecture, like seen in the figure \ref{fig: neural network}, meaning every node of the layer is connected to every node of the predecessor layer and every node of the successor layer.
Dense layers are pivotal in learning intricate patterns from data, their versatility allows them to be stacked where each layer captures different levels of data abstraction.
They work internally, like in the function above \ref{eq: NeuralFunction}.
\subsubsection{Convolutional Layer}
Convolutional layers, are used in image processing\cite{lecun_backpropagation_1989,szegedy_going_2014,krizhevsky_imagenet_2012}, unlike dense layers that try to analyse an image as a whole, but uses the concept of convolution to get a set of smaller pictures which depict isolated features of a picture.
A convolution describes how a function f is modified by a function g, for example in the discrete case: \ref{eq: convolution}.
\begin{equation}
(f \ast g)[n] = sum_{m=-\inf}^{\inf}f[m]\cdot g[n - m]
    \label{eq: convolution}]
\end{equation}
But how does this correspond to neural networks and image processing, foremost, we have not 1Dimensional data, but 2Dimensional pictures. \ref{eq: 2Dconvolution} \ref{fig: convolution matrix}
\begin{equation}
(f \ast g)[x,y] = \sum^{\inf}_{m=-\inf} \sum^{\inf}_{n=-\inf} f[m,n]\cdot g[x-m,y-n]
    \label{eq: 2Dconvolution}
\end{equation}
\input{figures/convolution.tikz}
For this, the singular weights are replaced by matrices of weights, so-called kernels or filters.
The output of the convolutional layer, are so-called feature maps, each of these contains the result of applying one filter to the input data.
These capture different aspects, like edges or textures.
There are two other variables in a convolutional layer, apart from the number of the number and size of filters and the activation function.
They are the stride and the padding, the stride describes how many steps the filter moves between successive convolutions.
The larger stride, the smaller the resulting filter map, and vice versa.
Of course, a larger stride results in less detail in a map.
Padding is an optional parameter which aids in maintaining information on the edge of an input and helps to keep the scale of a picture.
\subsubsection{Pooling Layer}
Pooling is an often used concept in Convolutional Neural Networks to reduce the size of the individual feature map produced by the convolutional layer, while conserving important details of the feature map.
Especially to produce an output which has a similar size as the input, a pooling layer can be avoided \cite{jain_supervised_2007}.
There are multiple types of pooling operations, but the most used ones are Max-Pooling \ref{fig: Max Pooling} and Average-Pooling \ref{fig: Average Pooling}.
Which operation is best depends on the type of data to be analysed and its pooling cardinality, which describes the number of extracted features.
There can be said, for smaller cardinalities a Max-Pooling should be used, for larger ones Average-Pooling \cite{boureau_theoretical_2010}.
In the complex layer terminology \cite{goodfellow_deep_2016} there are, the convolution stage, the detector stage (Which is just the usage of a nonlinear activation function) and the pool stage, these stages are sublayers of the large convolutional layer.
In the simple layer terminology, those three are all individual layers.
\input{figures/pooling.tikz}

Now, with the usage of the convolutional layer and the pooling layer, we can preprocess data to get better results with the dense layers, which we still need to classify our data.
\section{Neural Network Testing}\label{sec:neural-network-testing}
\section{Mutation-based Testing}\label{sec:mutation-based-testing}
\section{Spectrum Analysis}\label{sec:spectrum-analysis}
\section{DeepFault}\label{sec:deepfault}\cite{wong_dstar_2014}