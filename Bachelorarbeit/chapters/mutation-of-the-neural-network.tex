Now we move on to the second step in our approach: modifying our network.
We designed the function \texttt{run\_modification\_algorithm} in listing \ref{lst:mutation} and schematically shown it in \ref{fig:flowchart} for this purpose.

We hand over multiple parameters to the function.
The first parameter is \texttt{run}, which is used to be able to clearly identify the different runs of the experiment during the experimentation, it can be omitted in a production environment.
The next parameter is \texttt{modelname}, which is the name of the model that we want to modify and is used to identify the model and being able to load the model, and it associated data used in the analysis, as seen in the previous chapter \ref{sec:spectrum-based-fault-localisation}.
The next parameters are \texttt{train\_images, train\_labels, test\_images, test\_labels} the test data is needed to evaluate the model and perform the spectrum-based fault localisation and the training data is needed to retrain the model, if this options is chosen.

Next we can choose \texttt{analysis\_approach}, which is the similarity coefficient we want to use to rank our neurons.
The next parameter is \texttt{mutation\_function}, which is the mutation function we want to use to modify the weights and biases of the neurons, the mutation functions are discussed in the next subsection \ref{subsec:mutation-functions}.
The \texttt{value} is used to specify the value parameter of the mutation function, which is used to modify the weights and biases of the neurons.

The \texttt{old\_loss} and \texttt{old\_accuracy} are then used to save time by offloading the initial model evaluation required at the start of the function, which now only needs to be done once, rather than each time the function is used.
In a production environment, these parameters can be omitted and the evaluation would be performed in the function.

The parameter \texttt{train\_between\_iterations} is used to indicate whether we want to retrain the model or not, we can also specify \texttt{loss\_offset} and \texttt{loss\_accuracy} which is used to prevent the model from being terminated prematurely if the loss and accuracy are only slightly worse than the previous iteration, but may improve in the next iteration.
We also have the option to specify \texttt{regress\_loss} and \texttt{regress\_accuracy} which is used to indicate whether we want to regress the loss and accuracy, which is used in the break conditions we will describe in the next paragraph.

The break conditions are used to determine when the model should be terminated.
We have four different break conditions, which are used to compare the loss and accuracy and determine whether the model should be terminated, these can be seen in the listing \ref{lst:mutation} in lines 19 to 23.
We chosen them to see which condition has the greatest influence on the improvement of the model.
For the redability of the model we have omitted the data logging functions of the function in listing \ref{lst:mutation}, the full function can be found in the appendix \ref{lst:mutation_full}.

At line 7, \texttt{tf.random.set\_seed(42)} is included for evaluation purposes, to enable comparison of results from different runs, it would be omitted in a production environment.

\lstinputlisting[language=Python, linerange={9-14,23-34,45-54,58}, caption={Modification Algorithm}, label={lst:mutation}]{source_mutation/__init__.py}
\input{figures/flowchart.tikz}
%todo: add mutaiton functions
\subsection{Mutation Functions}\label{subsec:mutation-functions}
Now, let's discuss the mutation functions that we pass to our main function.
These functions allow us to modify the weights and biases of a neuron, which is similar to the theoretical representation of a neuron given in the equation \ref{eq: DenseLayer} the background chapter.
The basic functionality of the mutation functions is mostly the same.
We first isolate the layer from the model, which is the first entry in the coordinate tuple.
As seen in the two listing \ref{lst:weight_mod}, \ref{lst:bias_mod} which are the weight and bias modification functions, respectively.
Then, we obtain the weights, where the first entry represents the weights of the neuron and the second entry represents the biases.
The matrix can be mutated according to specific requirements.
To do this, isolate the entries that represent the specified neuron in the second entry of the coordinate and modify them accordingly.
Finally, reassign the weights back to the layer and return the model, which must then be recompiled.

\lstinputlisting[language=Python, linerange={4-33}, caption={Weight Modification Functions}, label={lst:weight_mod}]{source_mutation/utilities.py}
\lstinputlisting[language=Python, linerange={35-45}, caption={Bias Modification Functions}, label={lst:bias_mod}]{source_mutation/utilities.py}

Multiple mutation functions have been implemented for not only dense layers but also 2D Convolution Layers.
Mutation functions are used to assign a singular value to all weights or the bias of a neuron.
This value can either be fixed or random, derived from a uniform or Gaussian normal distribution.
For random values, each weight value can be assigned a new random number individually.
Additionally, scalar mutation functions in listing \ref{lst:scalar_weight_mod} and \ref{lst:scalar_bias_mod} can modify the existing values without assigning new ones.

\lstinputlisting[language=Python, linerange={68-81,115-127}, caption={Scalar Weight Modification Functions}, label={lst:scalar_weight_mod}]{source_mutation/utilities.py}
\lstinputlisting[language=Python, linerange={87-97}, caption={Scalar Bias Modification Functions}, label={lst:scalar_bias_mod}]{source_mutation/utilities.py}